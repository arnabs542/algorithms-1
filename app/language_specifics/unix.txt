decompress lzo file - 
lzop -d test.lzo

---------
zip_code visualizer 
https://www.maptechnica.com/zip-code-map/60606
------------------
# to do this, you need to be in a directory containing a Vagrantfile, example ~/vagrant/s_development-trusty/
vagrant up

vagrant ssh -p
vagrant halt   #shuts down VM and releases system memory of MAC OS X host, so other applications can use more
vagrant box list # lists vagrant commands
vagrant box remove s_development-trusty # To get rid of vagrant's cached version of the box (i.e. to start over with a new boxfile)
vagrant destroy #To get rid of the box permanently

ssh://vagrant@vagrant.krxd.net:2222/home/vagrant/.virtualenvs/pycharm/bin/python -u /home/vagrant/console/manage.py runserver 0:38080
----------------
in jira, to embed sql code , 
{code:sql}
select * from t;
{code}


to embed stack trace - 

{noformat}
stack trace
{noformat}


use these 2 as well - 
8.8.8.8
8.8.4.4
-------

ruby mine garbage collection options - 
-XX:InitialHeapSize=2g
-XX:MaxHeapSize=4g
-XX:ReservedCodeCacheSize=64m
-XX:+UseCodeCacheFlushing
-XX:+UseCompressedOops


pycharm - to increase heap space on mac osx 10.9 maverick 

you should copy the original pycharm.vmoptions file from the /Applications/pycharm<version>.app/Contents/bin/pycharm.vmoptions to ~/Library/Preferences/PyCharm50/pycharm.vmoptions, then modify the -Xmx setting

for intellij, its idea.vmoptions.

--------

pycharm - database execute command - cmd-enter 
how to set this when its not default key binding?

Console.Execute.Multiline  - execute current statment in multiline console; 

you can search in pycharm preferences windows as "execute"


cmd<F6>
when this is pressed on a table name, you can modify DDL/DML on it.

how to do \dt or \dn in pycharm to list tables within a schema?

----------
to determine number of cpu cores on a linux machine - 
more /proc/cpuinfo | grep cores

to prevent git pull from prompting for pwd, do this - 
git config -l
git config remote.origin.url git@gitlab.castlighthealth.com:core-data-engineering/provider_directory.git

-------------------


put -r final_de-id_data

-------------------
cat claims_table_schema.txt | awk -F "|" '{print $1}'

counts blank lines column-wise
cat test1.psv | awk -F "|" '{print $3}' | grep -e "^$" | wc -l


enumerate line numbers in awk to a csv file (attach id column)- 
cat restaurant_test.csv | awk '{print NR", " $0}'

substitute whole file text with underscore (regexp find and replace in-place in file ) , alternatively can also use awk and gsub
perl -pi -e 's/" ,"/","/g' file2
perl -pi -e 's/Anaplasmosis and Ehrlichiosis/Anaplasmosis_and_Ehrlichiosis/g' Infectious_Disease_Cases_by_County__Year__and_Sex__2002-2014_w_id2.csv
--------

bulk find and replace in csv file - all occurences of ,"", with ,, in parallel for fast speed-ups on large number of rows (lines)
cat file.csv | parallel --keep-order --gnu --pipe 'sed "s/\"\"//g"' > file.csv.nulls 

same thing wrapped in python - 
def convert_empty_strings_to_null(file_path, output_file_path):
    sed_cmd = 'sed "s/\\"\\"//g"'
    parallel_sed_cmd = """cat {0} | parallel --keep-order --gnu --pipe '{1}' > {2}""".format(
        file_path,
        sed_cmd,
        output_file_path
    )
-------------
header is copied to on bam0 (DB server)
/data/gpfdist/your_location

dump table from postgres to csv - 
SqlUtils.dump_via_gpfdist_or_copy :rx_esi_web_services_form_options, "/home/ssatish/rx_esi_tables_data/v3/rx_esi_web_services_form_options.csv"


data is copied to zap-master machine (if you specify :remote=>true to dump via gpfdist or copy)
location ('gpfdist://zap-master01-int:45808/medical_claims.xlsx')
that is , on zap-master01, at /gpdump/medical_claims.xlsx dumping with chmod 600 as user gpadmin

Running R_LIBS=/big_home/users/ssatish_shared/persistent_data/R_LIBS RAILS_ENV=development REVENGE_DATA_DIR=/big_home/users/ssatish_shared/revenge-sync/R/third_party/src/output_data REVENGE_ROOT_DIR=/big_home/users/ssatish_shared/revenge-sync /usr/bin/env Rscript -e 'library(tools); write_PACKAGES(type="source")'  2>&1 

--------
connect as some other user to increase database connection limit - 
RAILS_ENV=other_db_full OVERRIDE_DB=ssatish_full script/console
these options are there in database.yml

once i drop ssatish_full, load_from_ci gives error taht ssatish_full does nto exist. how to create schema?
RAILS_ENV=bam_full rake data:create_db
RAILS_ENV=bam_full rake db:drop && rake data:create_db

after above step which creates ssatish_full, load data into the DB and create views with - 
rake data:load_from_ci[humana,true,true]
above takes about 5 hours (10 AM to 3 PM) for cigna_web_services, to load all of reference, warehouse tables. 

now run the pricing pipeline with 
RAILS_ENV=bam_full ./script/process -Ab cigna_web_services
for cigna, this takes 2.5 hours to get to exporting app_priceables 

when you do load_from_ci and schema doesnt exist "bsbc_ma" and fails , you can 
data:create_layer[layer_name]

---------------------
directly connecting to release database (palladium which is going to come into App deploy on March 13)
RAILS_ENV=release RELEASE=cerium1 DB_USER=revenge script/dbconsole
RAILS_ENV=pre_release RELEASE=barium1 script/console

run pricing pipeline for 1 bucket after creating table schemas - 
RAILS_ENV=bam_full script/process -Ab phs_php_fc

runs except(init, parsing), synthesis (s), full_pricing (f), export(e), granite(g) 
RAILS_ENV=bam_full script/process  -bsfeg cigna_web_services

run only QA tests for export phase for cigna layer - 
RAILS_ENV=bam_full script/process -eb cigna_web_services -Q

runs only 1 export phase test class for export layer for cigna
RAILS_ENV=bam_full QA_PERSIST=true QA_CLASS=ConfirmHighProfileIssuesDontReturn script/process -eb cigna_web_services -Q

sometimes to simulate unit_tests outside of running pricing pipeline - 
script/console
include Test::Unit::Assertions
then you can use assert_* methods

to continue from a failed phase without repeating previous steps - 
RAILS_ENV=bam_full script/process -Ab phs_php_fc --pending

if you get error not finding global.runs with above pending cmd, 
run rake views:load to load all views and try again 

RAILS_ENV=bam_full vendor/jruby-1.6.2/bin/jruby --server -J-Xms18G -J-Xmx18G --1.9 script/synthesis --layer phs_php_fc --tick-file tmp.txt

disk space check utility 
/usr/lib64/nagios/plugins/check_disk -w 5% -c 4% -p /data
w = warning 
c = critical

gpssh -f hostfile df -h
check disk space usage on DB host
---------------------------------
tmux new -s 1
https://robots.thoughtbot.com/a-tmux-crash-course

tmux attach -t 1
tmux switch -t 1
tmux list-sessions


go up in tmux buffer - 
ctrl B [

#alternative to tmux and comes by default on linux machines - 
screen -ls

#to see all screen sessions on a machine
ls -laR /var/run/screen/

#to switch to a particular screen
screen -x screen_id
-------------------
ruby mine shortcuts 
*******************
cntrl-tab = DB connection and other options

-------------
script/console - 
ruby / rails controller code
****************************


p = User.first
p.save
p.reload
u = Tweet.user

tweets = Tweet.find_by_user_id(2) => returns only the first one. not all the tweets 
tweets = Tweet.find_all_by_user_id(2);     --> use this to display all tweets in users:show.html.erb

logger.debug "#{@user}"
User.find_by_id(2).user_name

find_by_sql("select * from User")

controller code debug - 
logger.debug "suhas #{@following.attributes.inspect}"
---------------------
rails models - 
*********************
changing table schema - generate migrations and add up and down method definitions
./script/generate migration AddUserIdToTweet 

real revenge code migration:
on smash3: RAILS_ENV=bam_full script/generate wh_migration add_rx_esi_tables antimony

bucket specific mappings - 
script/dbconsole -b cigna_swy

script/console 
Db.use_bucket_schema :aetna
script/console -b aetna
when accessing different schema from ruby, use Db.with_search_path (and call methods in $REVENGE_HOME/lib/db.rb)

to change a schema in revenge ruby code:
Db.with_layer :bcbs_altnet do
end

Sql.row_count
rvng util function - to get number of rows of a table
----------------
rails views - 
*************
login.html.erb:				<%= link_to "New User Singup", :action => "new" %>
"link_to" command bypasses controller and goes directly  with a HTTP GET requesst to the view mentioned in :action, ie, new.html.erb

on the other hand, "button_to" cmd goes to the :action method() in this case, new() method in controller


variable substitution in views - 
<legend>Welcome to Revenge Twitter <%= @user.user_name%>  !  </legend>

form_for(Tweet.new)
f.text_area
-------------------------
greenplum check query plan - 
https://bam0:28080/
un: gpmon
pwd: BamRevenge
ERROR:  Out of memory  (seg15 slice6 den-bam4:40003 pid=46763)


postgresql - 
connecting to revenge DBs from ruby mine - 
host: zap-master01.ch.int
database: release_palladium1
user: revenge
pwd: BamRevenge

su - gpadmin (password BamRevenge)

gpstart should start the postgresSQL server. 
************
SQL RDBMS schema design - 
given the very low costs of storage versus the improved performance characteristics of querying a single table and not having to deal with SQL statements that operate across six tables for every operation. This is a small price to pay.

 it does mean that the name has to be fixed up in thousands of tables if it ever changes. Since this is likely to happen very rarely, this is probably acceptable especially if we schedule renames to be done by a cron job during offpeak ours On the other hand, replicating the member count is just asking for trouble since there are frequent updates to this and updates have to happen on multiple tables simultaneously and atomically. 

Database denormalization (storing redundant information in extra columns) is the kind of performance optimization that should be carried out as a last resort after trying things like creating database indexes, using SQL views and implementing application specific in-memory caching. 
-------------
show tables - (display tables)
\dt

select * from pg_catalog.pg_tables;

select * from tweets where user_id =2;

jdbc:mysql://den-signoff01-db01:3306/ventana_densignoff01
---------
./script/generate scaffold user user_name:string hashed_password:string salt:string
../../../script/generate scaffold tweet text:string timestamp:datetime

after this step above, it generates the db migration file which you have to run with "rake db:migrate" to create the table 
>script/generate controller home index
  --> creates home controller with index view
-------------------
vpn mac problems - kill this restart and restart vpn application
ps aux | grep racoon

--------
vi editor reize split windows - 

:resize 60
ctrl W > 
ctrl W < 
:resize 5+
:resize 5-

In Vim, after enabling search highlighting with: 
:set hlsearch
, you can check with the following key sequence in normal mode(you can hit ESC twice to be sure): 

/, Ctrl-v, Tab, 
then hit Enter. Also, you can convert tabs to 2 spaces by these commands in Vim: 

:set tabstop=2 expandtab 
and then 

:retab.

vim editor - how to find and replace multiple spaces in a line with something else - escape the '+' regex
%s/\s\+/= NULLIF(trim(/gc 
------
vim find and replace with regexp - line separations not preserving DB row integrity

%s/\n^\([^\d\d\d\d\d].*\)\n/ \1/gc

to enable greek alphabet and superscript/subscript in vim, use :digraphs and choose anything
eg - while typing in insert mode, if you press crtl K and then 2s, it will create 2 as a subscript,
like L₂. 
http://www.alecjacobson.com/weblog/?p=443
--------
vim show invisible ascii character at cursor position
http://vim.wikia.com/wiki/Showing_the_ASCII_value_of_the_current_character

********running a single unit test **************
ruby -I lib:test path/to/test.rb

running single unit test within a .rb test file - 
ruby -I lib:test test/unit/continuous_integration_test.rb -n class_method_compare_phase_version_with_file_



rake code_sync:init_auto_sync
to run auto sync

ps | grep rsync | grep -v grep
check if rsync background process is still running or not 

********************
weather on terminal os x - 
curl -4 http://wttr.in/SFO

frozen terminal shell due to lost vpn connection in mac os x - how to make it responsive again
<return> ~.
-------------
map.connect 'user', :controller => "users", :action => "logout"
/user path is mapped to above method & controller 
=======================================================
puts $:  - ruby checks paths in this for libs

irb - interactive ruby shell
-----------
 rvm info
 gem list
 rvm gemset list
 rvm use 1.9.3@global
 rvm gemset list
 gem list
ruby 2.2.0 on mac os x default is incompatible with CH code base. install ruby 1.9.3p286 with
rvm reinstall 1.9.3
rvm install ruby-1.9.3-p551
rvm use 1.9.3-p551
-----------
OSX commands & shortcuts - 
************************
switch gvim tabs in mac - cmd-shift-{  and cmd-shift-}
switch terminal tabs - cmd-shift-left
switch terminal windows - cmd-1
Error running 'requirements_osx_brew_libs_install gcc46',
showing last 15 lines of /Users/ssatish/.rvm/log/1423680094_ruby-1.9.3-p551/package_install_gcc46.log

F11 - show desktop - like windows-D

scanned documents from scanner are in Pictures folder as Scan.jpeg

remote gvim 
gvim scp://mapr@10.10.100.100//opt/mapr/pig/pig-0.10/test/utils/pigmix/scripts/L1.pig

word count in gvim - 
press g, then <ctrl> g
-------------
ssh smash3.ch.int

to install a partiular ruby depndency like bundler-1.3.5 - 
gem install bundler -v1.3.5

rake gems:unpack
gem install union_find --version "= 0.0.2"

to install gem in local directory  instead of global sudo /usr/rvm/* path,
gem install union_find --user-install
install s it under ~/.gem/
then copy it to vendor directory

gem list --local

force use particular version of rake - 
rake _0.9.2.2_ code_sync:install


-------------
postgre SQL
To have launchd start postgresql at login:
    ln -sfv /usr/local/opt/postgresql/*.plist ~/Library/LaunchAgents
Then to load postgresql now:
    launchctl load ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist
Or, if you don't want/need launchctl, you can just run:
    postgres -D /usr/local/var/postgres

launchctl limit maxfiles
//to see how many maxfiles can be watched (need to restart mac OSX after that change)
//stdout -> maxfiles    200000          200000

sudo launchctl limit maxfiles 1000000 1000000

https://superuser.com/questions/302754/increase-the-maximum-number-of-open-file-descriptors-in-snow-leopard
To make this permanent (i.e not reset when you reboot), create /etc/launchd.conf containing:
limit maxfiles 1000000 1000000

===================================MAPR=======================================================
make package-hbase HBASE_DIST=/home/ssatish/git_repos/hbase/0.94.3/target/hbase-0.94.3-SNAPSHOT-security.tar.gz 
above resolves build issue with make package-hbase

when name of a DISTR changes use below syntax for all ecosystem packages- 
make mapr-sqoop-internal SQOOP_IMAGE_DIST=/home/ssatish/ecosystem/sqoop/build SQOOP_IMAGE_NAME=sqoop-1.4.2-mapr-SNAPSHOT.bin__hadoop-0.20.2-2.1.2.tar.gz
-------------------------------------------------------
sudo update-alternatives --config java
sets java versions
----------------------------------------------------
Observation - map-reduce job was stuck and not progressing. 
Solution - No RAM left on desktop device. Diagnosed with the help of MapR gui where tasktracker was stuck in pending mode. Reduced memory for mfs from 30% to 20%  in warden.conf
---------------------------------------
eclipse short cuts
==================
ctrl shift G  - shows all uses of function calls
ctrl shift T  - opens new java file within eclipse
F3 - shows all class variables
---------------------------------------
mercurial commands- 
==================
hg init (creates .hg dir in .)
hg clone ssh://mapr@10.250.1.5/releases/trunk trunk2
(pwd is maprwins)
hg branches  - show all branches
hg checkout v3.1.1
hg fetch   (svn update+ merge, cmd has to be setup in .hgrc)
hg diff
hg stat
hg pull
hg update
hg commit
hg push
hg heads (to see heads)
hg revert

discard local changes and pull merge - 
hg update -r <branch_name> -C

mercurial discard unpushed but committed changes - 

---------------------------------------
git commands-
============
git branch
git checkout branch-1.4.2-mapr
git status
git add ivy.xml
git add */*.java   - add all modified java files in repo in different directories
git add .   - will add everything
git commit -m "BUILD - changed the dependency from com.mapr.hbase to org.apache.hbase"
git commit -a -m "Eco # PIG-3109" //does git add + git commit together of all tracked files (could be 100s of files)

amending a commit before push - takes you to git staging directory file where you can manually enter details, add more files, remove files added before pushing
git commit --amend --date "Thu, 13 Mar 2014 15:21:30 -0700"
git commit --amend --author "Romain Rigaux <romain@cloudera.com>"


git push 
git push origin 0.11-mapr  //push pwd onto 0.11-mapr branch

git log
git format-patch -1 <commit#>
git format-patch <commit#> - creates *.patch of all patches after that commit #
git apply < 0001-PIG-2251-PIG-leaks-Zookeeper-connections-when-using-.patch
git diff CHANGES.txt
git diff --cached

cloudera review board needs below cmd for code review submission requests- 
git diff --full-index


on branch cadmium1, u want to see commits on it since the fork-off point from master - do this 
git diff duplo-greenplumize...HEAD
git diff master...HEAD

getting git diff between 2  non-consecutive commits - 
git diff --full-index 3492128594d48badcac204af09e08aca6a6d07f2 385163cb625b9f0a012fd09b724f659a252a272f > HUE-2271.patch.2
where 349 is earlier commit and 385 is the latest commit - most recent in time

hive jira - 
git diff --cached --no-prefix > HIVE-7821.4-spark.patch

to create git diff by coloring white spaces with red - 
git config color.diff.whitespace "red reverse"
git diff --color | less -R
--------------

raw path to csv files uploaded on github (since html of large files get truncated on github for files larger than 25 MB)
https://raw.githubusercontent.com/suhassatish/datasets/master/TermDocumentFrequency.csv

pretty git log msg - 
git config --global alias.lg "log --color --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)<%an>%Creset' --abbrev-commit"
---------
if in git diff, ^M shows up, you have to convert al ^M (CR or CRLF) to LF in git handled repos with option - 
git config --global core.whitespace cr-at-eol
git config --global http.sslVerify falsegit config --global commit.gpgsign true


git config --global push.gpgsign if-asked
git config --list | grep sign

git config --global user.email "suhas.satish@gmail.com"

git branch -a  //lists all branches - remote+ local
git checkout flume-1.4
git branch -v   // lists last checkin on this branch

delete a local git branch - 
git branch -D 1b13f552f4be973c55e2a0f0f6008127ff5aa828

DELETING a remote branch 
git push -u  origin  :branch-1.4.4-mapr

git apply --check < blah.patch
instead of applying, checks if a patch is applicable

git apply --reject < blah.patch
leave the rejected hunks in corresponding *.rej files 

CREATING new git branch :
git checkout -b release-1.4.0-mapr
<apply mapr specific patches and make sure it builds>
git push -u origin  release-1.4.0-mapr  //Branch release-1.4.0-mapr set up to track remote branch release-1.4.0-mapr from origin. creates new  upstream remote repository.

see which branch git HEAD is pointing to  - 
cat .git/HEAD
git show-ref --heads -s  
shows tips of branches (heads ) of all branches checkout out


forking new git branch 
git clone -n git@github.com:mapr/private-sqoop.git  sqoop-1.4.4-mapr
cd sqoop-1.4.4-mapr
git checkout branch-1.4.4
git checkout -b branch-1.4.4-mapr    //creates new branch-1.4.4-mapr form apache branch-1.4.4
git push -u origin branch-1.4.4-mapr
-------------------------------------
to discard changes in working directory - use
git checkout -- desktop/libs/hadoop/src/hadoop/conf.py

to discard all unstaged changes, kinda like git checkout -- * do following - 
git stash save --keep-index
git stash show -p stash@{0} > stash.diff
git stash drop

# give a name to the stash
git stash save "refresh_matcher schema - no matcher_schema in search_path"
----------------------------------
checkout to 2 revisions older for unmerged change of branch when both are modified
git checkout HEAD~2 apps/hbase/src/hbase/api.py

git clone git@github.com:mapr/private-pig.git private-pig
cd private-sqoop
wget https://issues.apache.org/jira/secure/attachment/12563764/SQOOP-818.patch
patch -p1 < SQOOP-818.patch
ant clean compile	# compile code
ant package-all		# build jars
ant tar				#create release tar ball

GIT SPARSE CHECKOUT ONLY 1 file - 

delete file from file system. then do 
git checkout a690149de1dc69fce38dd0dbf83fef1f74a467d5 .  (will pull all changes till and including this checkin. changes after this point in the current dir will be lost)
checkout the corresponding version of that file that you want into the current directory and in the process, throws
away local edits to the changed index which shows "file modified." 

Commit by retaining original timestamp and mesage and author name (uses commit# of original patch)- 
git commit CHANGES.txt src/org/apache/pig/backend/hadoop/executionengine/HExecutionEngine.java   --author="Cheolsoo Park" -C c8fcaaad00d3e66725fb603c856b4fc0c60ae17d

git commit -m "HUE-1631-oozie-Support-JobTracker-HA-in-workflows" --author="Abraham Elmahrek" --date="Thu, 10 Oct 2013 13:48:40 -0700"

undo git add (unadd / subtract) by removing from commit tracking on index
git reset HEAD TableDefWriter.java
Unstaged changes after reset:
remove file from git
git rm bin/n* bin/p* bin/s* bin/w*

undo git commit - 
git reset --soft HEAD~ 

A URL of the form https://github.com/<owner>/<project>/commit/<hash> will show you the changes introduced in that commit. For example here's a recent bugfix I made to one of my projects on GitHub:
https://github.com/jerith666/git-graph/commit/35e32b6a00dec02ae7d7c45c6b7106779a124685
You can also shorten the hash to any unique prefix, like so:
https://github.com/jerith666/git-graph/commit/35e32b

http://www.dmo.ca/blog/20080307124544/
renaming remote git branch (-m for move)
git branch -m master old-master
git push origin old-master:refs/heads/old-master

git branch remove delete tracking - 
git branch -D hue-3.5.0-mapr
git stash 
save your dirty changes to push stack before checking out different branch
git stash list     //lists stashes or entries on stack
git stash apply  //to pop changes
http://git-scm.com/book/en/Git-Tools-Stashing

#to find the branch on which a particular commit resides - 
git branch --contains <commit> (returns branch name if current branch checkedout has the commit, else returns empty string)
git reflog show --all | grep a871742

--------------------------------------
git remote
shows which is your remote server(s)

git remote rename pb paul

git remote add [shortname] [url]
git remote add upstream https://github.com/romainr/hadoop-tutorials-examples.git
track a remote git branch by adding the remote tag to the github server to track it.

git fetch upstream
# Fetches any new changes from the original repository

git merge upstream/master
Merges any changes fetched into your working files

git push [remote-name] [branch-name]
git push origin master
(can now see upstream changes in my forked branch)
-----------------------------------
git pull requests
https://help.github.com/articles/using-pull-requests
also see child pages -
Merging a Pull Request
Closing a Pull Request
Tidying up Pull Requests
-----------------------------
squashing git commit (last 4)
http://gitready.com/advanced/2009/02/10/squashing-commits-with-rebase.html

1) 
git rebase -i HEAD~4

file opens - 
change pick to squash for all but the most recent commit - 
pick 01d1124 Adding license
squash 6340aaa Moving license into its own file
squash ebfd367 Jekyll has become self-aware.
squash 30e0ccb Changed the tagline in the binary, too.

Always Squash the newer into the older commit

then push to remote
2)
git push
----------------------------------
git pull --rebase (does not create a merge after pulling in someone else's change when you commit your change)

pull from a certain branch
git pull git@git.castlighthealth.com:repositories/revenge_twitter.git suhas

-----------------------------------------------------
from Aditya Kishore's email/brown bag
git rebase merge - best practices 


git checkout main      # switch to main branch
git pull               # sync your local main branch with the remote main branch
git checkout private   # switch back to private
git rebase main        # rebase your changes on top of main
git checkout main      # switch to main branch again
git merge private      # merge you changes on top of main
<build_the_merged_code>
git push               # push your changes upstream

other useful rebase options - 
git rebase --continue
git rebase --skip
git rebase --abort
----------------------------------------------------
easy and hard ways to squash several git commits into a single commit in master -
http://makandracards.com/makandra/527-squash-several-git-commits-into-a-single-commit

Easy mode: Reset your feature branch to the master state
**********
The easiest way to turn multiple commits in a feature branch into a single commit is to reset the feature branch changes in the master and commit everything again.

# Switch to the master branch and make sure you are up to date.
git checkout master
git fetch # this may be necessary (depending on your git config) to receive updates on origin/master
git pull

# Merge the feature branch into the master branch.
git merge feature_branch

# Reset the master branch to origin's state.
git reset origin/master

# Git now considers all changes as unstaged changes.
# We can add these changes as one commit.
# Adding . will also add untracked files.
git add --all
git commit
----------
--how do I merge changes to a single file, rather than merging commits?

A simple command already solved the problem for me if I assume that all changes are committed in both branches A and B:

git checkout A

git checkout --patch B f
The first command switches into branch A, into where I want to merge B's version of the file f. The second command patches the file f with f of HEAD of B. You may even accept/discard single parts of the patch. Instead of B you can specify any commit here, it does not have to be HEAD.

-----------------------------------------------------
Pig tool plugin for vim - snippets 

Registering all your .jar files can be a pain, this will make it easier:

regloc : `register local`. Searches for *.jar in ./ and registers them
reglib : `register lib`. Searches for *.jar in /usr/lib/pig and registers them
Joins in PigLatin know several types, depending on inner or outer joins:
l
ji : `join inner`. Select 1 through 4 for normal, skewed, replicated or merge inner join
jo : `join outer`. Select 1 through 3 for normal, skewed, replicated oiter join
Compression options require you to set multiple mapred options:

setsc : `set snappy compression`. Outputs all options for job output, map output, tmpfile compression
----------------------------------------------------------
/opt/mapr/server/configure.sh
configures cldb (NameNode) and Zookeeper IP@
/opt/mapr/server/configure.sh -C 10.10.30.152:7222 -Z 10.10.30.152:5181 -u mapr

cldb doesnt come up in warden.log if disksetup done after configure.sh is not performed correctly. re-run disksetup if you see this error - 

configure.sh -R -hadoop 1 -RM 10.10.30.81
-----------------------------------------------------------
ps -auxww | grep mapr | grep "attempt"
displays amount of jvm heap memory (RAM) size for each map-reduce task attempted
Its displayed in the -xmx field of stdout
-----------------------------------------------------------
ls -l lists the following columns - 
file mode, number of links, owner name, group name, MAC label,
     number of bytes in the file, abbreviated month, day-of-month file was
     last modified, hour file last modified, minute file last modified, and
     the pathname.
-----------------------------------------------------------
HADOOP_HEAPSIZE and other hadoop specific environment variables set here 
/opt/mapr/hadoop/hadoop-0.20.2/conf/hadoop-env.sh
-----------------------------------------------------------
/opt/mapr/hadoop/hadoop-0.20.2/conf/mapred-site.xml
has below paths - 
/tmp/mapr-hadoop/mapred/local   where mapReduce stores job jar, xml files and creates work dirs for tasks
had to set chmod777 permissions for this directory  /tmp/mapr-hadoop/mapred/local/taskTracker/ssatish/jobcache
-----------------------------------------------------------
chmod fields  - There are effective-userID (sets file permissions) and groupID for each owner. 
http://linuxfocus.berlios.de/English/January1999/article77.html
owner (rwx/s) | group(rwx/s) | others(rwx/s)

s bit  set  on file-owner - used mainly for binaries
This causes the file to be executed under the user-ID of the user that owns the file rather than the user that executes the file
 s bit set on group - Executable files the that have the s-bit set on the group run under the group-ID of the file owner.
-----------------------------------------------------------
watch "ps -eaf | grep -i attempt" 

-----------------------------------------------------------
pig grunt - setting number of mappers from within pig 
set mapred.map.task 4;
-----------------------------------------------------------
terasort IP @ machine
http://10.10.100.99:1234/job/terasort/configure
-----------------------------------------------------------
this is how to see the vim runtime path from within vim - 
:set runtimepath?
  runtimepath=~/.vim,~/.vim/bundle/snipmate-pig,~/.vim/bundle/tlib_vim,~/.vim/bundle/vim
-addon-mw-utils,~/.vim/bundle/vim-sensible,~/.vim/bundle/vim-snippets,/var/lib/vim/addon
s,/usr/share/vim/vimfiles,/usr/share/vim/vim73,/usr/share/vim/vimfiles/after,/var/lib/vi
m/addons/after,~/.vim/after
-----------------------------------------------------------
finding and searching installed package names on debian ubuntu .deb 
dpkg -L exuberant-ctags

-------------------------------------------
generate vim ctags for java - in top level project directory , run the following command - 
find *  -type d -exec ~/.vim/dirtags {} \;
bottom of this page has steps- 
http://ctags.sourceforge.net/faq.html

To check if tags are set ofr not, within vi, 
:set tags?
-------------------------------------------
comments in code check-ins:
MapR: for mapR bug fixes but dont use details. Say feature enhancements.
Eco-#(Pig-2222) for Jira patch migrations
Build: For a specific build #
-------------------------------------------
running single Pig test - pass test name to the proprty testcase
ant -Dtestcase=TestRegisteredJarVisibility clean test
-------------------------------------------
downloading and installing cpan perl modules using utility cpanminus
cpanm Module::Name
-------------------------------------------
su mapr -p -c "pig -f ./ex1_mapr.pig"
-------------------------------------------
/etc/passwd
has userID, groupID and group #. It also has default shell sh , csh or bash for different users
-------------------------------------------
to add new user, use 
man adduser
-------------------------------------------
authenticate ssh
 ssh -T git@github.com
-------------------------------------------

html.erb syntax highlighting in vim - 
:set filetype=eruby.html
-------------------------------------------
To know if a package is installed or not on an ubuntu machine - 
example: apt-cache policy mapr-oozie
syntax: apt-cache policy <pkg-name>
-------------------------------------------
install oozie client directly - 
download .deb pkg from 
http://apt.qa.lab/opensource/dists/binary/
sudo dpkg -i /home/ssatish/Downloads/mapr-oozie-internal-3.3.0.19888_all.deb
-------------------------------------------
java -verbose  --> tells which package the particular function call is getting picked from

yum provides/whatprovides \*bin/ls  -->  tells (on centOS/redhat) what packages use this particular utility or .jar file as a dependency

jvm performance tuning - 
use -verbosegc to see whats happening with garbage collection which is the most time consuming event. 
observe numbers in "Full GC" messages
-------------------------------------------
search for classes and methods existence within a jar file
javap -classpath /opt/mapr/hbase/hbase-0.94.5/lib/guava-11.0.2.jar  -c com.google.common.collect.ImmutableSet | grep 'public static com.google.common.collect.ImmutableSet of'

/usr/lib/jvm/java-6-sun/bin/javap -classpath ./lib/flume-hdfs-sink-1.3.1-mapr.jar  -c -private org.apache.flume.sink.hdfs.HDFSEventSink > log.txt

cat /etc/issue   --> tells which distribution of linux is being run on a machine

jar -tvf some.jar > some.txt
//extracts all class file names contained within the jar
-------------------------------------------
http://wiki.apache.org/hadoop/WordCount

javac -cp `hadoop classpath` WordCount.java -d .

--------------------------------------------
test hadoop security impersonation - 

javac -cp `hadoop classpath` TestImpersonation.java
compiles the java file into TestImpersonation.class

to run it -
java -cp `hadoop classpath` TestImpersonation.class
throws Caused by: java.lang.ClassNotFoundException: TestImpersonation

make sure its in a path picked up by `hadoop classpath`
sudo cp TestImpersonation.class /opt/mapr/hadoop/hadoop-0.20.2/bin/../conf

java -cp `hadoop classpath` TestImpersonation ssatish /user/ssatish/test_dir
got this -> Caused by: java.lang.UnsatisfiedLinkError: no MapRClient in java.library.path
 
so, this below solved it -
/usr/local/java/jdk1.7.0_55/bin/java -Djava.library.path=/opt/mapr/lib -cp `hadoop classpath` TestImpersonation ssatish /user/ssatish/test_dir
stdout-
14/06/30 15:31:56 INFO security.JniBasedUnixGroupsMappingWithFallback: Falling back to shell based
ssatish via mapr
14/06/30 15:31:56 INFO fs.MapRFileSystem: User mapr is impersonating user ssatish

---------------------------------------------------
running a sample map-reduce job
attached eclipse debugger to hadoop and ran the below file - 
hadoop jar wc.jar org.myorg.WordCount /user/root/test/q1/000000_0 /user/root/o1
                                        <wordCount_input>  <wordCount_output>    
eclipse remote debugging - 
attach bin/pig.debug  args for eclispe remote debug
then in eclipse - 
Run -> Debug Configurations -> Remote Java Application -> Connection Properties:
localhost
port 1045 - should match whatever is in bin/pig.debug

mapr wordcount example - 
 hadoop jar /opt/mapr/hadoop/hadoop-0.20.2/hadoop-0.20.2-dev-examples.jar wordcount /myvolume/in /myvolume/out
-------------------------------------------
recursive grep with syntax color highlighting
grep -rn "MAPRED_MAP_TASK_JAVA_OPTS" *
-------------------------------------------

CDH4 hadoop conf dir - 
/etc/alternatives/hadoop-conf

hadoop classpath has 
$HADOOP_CONF_DIR:$CLASSPATH
-------------------------------------------
see the network socket ports that are being used 
ping qa-node57
netstat -ant
-------------------------------------------
dpkg -l | grep mapr
dpkg cmd lists status as a 2 character string which means as follows - 

First character: The possible value for the first character. The first character signifies the desired state, like we (or some user) is marking the package for installation

u: Unknown (an unknown state)
i: Install (marked for installation)
r: Remove (marked for removal)
p: Purge (marked for purging)
h: Hold
Second Character: The second character signifies the current state, whether it is installed or not. The possible values are

n: Not- The package is not installed
i: Inst – The package is successfully installed
c: Cfg-files – Configuration files are present
u: Unpacked- The package is stilled unpacked
f: Failed-cfg- Failed to remove configuration files
h: Half-inst- The package is only partially installed
W: trig-aWait
t: Trig-pend

To remove mapr-client to install wls_mapr_client, all the dependencies have to be removed as well. 
mapr-client            

dependencies- 
------------
mapr-hbase          
mapr-hive           
mapr-oozie-internal 
mapr-sqoop

to show dependencies 
dpkg -I package.deb
or
apt-cache showpkg package-name

/etc/apt/sources.list
deb http://package.mapr.com/releases/ecosystem/ubuntu binary/

redhat - /etc/yum.repos.d/mapreco.repo -
baseurl=http://package.mapr.com/releases/ecosystem/redhat
baseurl=http://yum.qa.lab/opensource

force remove broken ubuntu package - 
rm /var/lib/dpkg/info/mapr-oozie-internal.prerm 
dpkg --remove --force-remove-reinstreq mapr-oozie-internal
apt-get purge mapr-oozie-internal

-------------------------------------------
cat /opt/mapr/MapRBuildVersion  (server side)

should match the same version on server side and hadoop mapr-client side.


otherwise, hadoop fs -ls / will not work and you see the following error - 
2013-07-09 14:57:17,5143 ERROR Client fs/client/fileclient/cc/client.cc:307 Thread: 140666126419712 Failed to initialize client for cluster secure133, error Connection reset by peer(104)
ls: Could not create FileClient
-------------------------------------------
maprlogin password
will create ticket in 
/tmp/maprticket_0

maprlogin print
prints ticket credentials
-------------------------------------------
dpkg --list |grep "^rc" | cut -d " " -f 3 | xargs sudo dpkg --purge
sudo dpkg --purge --force-all package

if purge marked pkg as pc, then do this - 
rm /var/lib/dpkg/info/mapr-cldb.list
rm /var/lib/dpkg/info/mapr-cldb.postrm
dpkg --purge --force-remove-reinstreq mapr-cldb

remove packages marked as rc completely
-------------------------------------------
bulk search find and replace across multiple files in unix directory tree - example - replace flume 1.4.0 to 1.4.0-mapr in 52 pom.xml files under flume project
find /path/to/start/from/ -type f | xargs perl -pi -e 's/applicationX/applicationY/g'


find all jpg files recursively and sort by size based on 5th column to find biggest largest; 
find ./ -name "*.jpg" | xargs ls -ltrh | sort -k 5

copy from different recursive directories into 1 location - 
find ./ -name "*_1024.jpg" | xargs -I{} cp {} /Users/ssatish/Dropbox/Pics/2016/4_Italy_Jan23-Feb1/tmp/

recursively search all python files in project heirarchy to see if a particular method is being used anywhere - 
find ./ "*.py" | xargs -I{} grep -rl "generate_network_table_mbs_pd" {}
-------------------------------------------
add user mapr to sudoers list - 
1. login as root
2. visudo   //opens /etc/sudoers.tmp on ubuntu m/c
3. mapr ALL=(ALL) ALL
-------------------------------------------
adding user ssatish (renaming user1 who already has UID=1000, GID=1000) to cluster nodes with the same UID and GID as on client m/c
view all UIDs in /etc/passwd
/etc/group stores group names and info
usermod -l  ssatish  user1
usermod -m -d /home/ssatish ssatish
login as root: chgrp root /home/user1
chmod -R 755 /home/ssatish
grep ssatish /etc/shadow
grep ssatish /etc/passwd
login as root: usermod -a -G shadow ssatish
login as root: groupmod -n ssatish user1

groupmod -g 500 mapr    (changes group ID of mapr to 500)
usermod -u 500 mapr (changes the user ID of mapr to 500)

login as root: id should print
uid=1000(ssatish) gid=1000(ssatish) groups=1000(ssatish),500(shadow) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023

groupadd ssatish -g 1000
useradd ssatish -u 1000 -g 1000

on mac os x , the same cmds dont work , use this instead - 

sudo dscl . -create /Users/revenge UserShell /bin/bash
sudo dscl . -create /Users/revenge UniqueID "4005"
sudo dscl . -create /Users/revenge PrimaryGroupID "100"
sudo dscl . -create /Users/revenge NFSHomeDirectory /Users/revenge
-------------------------------------------
java multi threaded process hang debug - 

kill -3  <pID>   --> prints the events in the background of the hung thread
kill -l | more
 1) SIGHUP	 2) SIGINT	 3) SIGQUIT	 4) SIGILL	 5) SIGTRAP
 6) SIGABRT	 7) SIGBUS	 8) SIGFPE	 9) SIGKILL	10) SIGUSR1
11) SIGSEGV	12) SIGUSR2	13) SIGPIPE	14) SIGALRM	15) SIGTERM
16) SIGSTKFLT	17) SIGCHLD	18) SIGCONT	19) SIGSTOP	20) SIGTSTP
21) SIGTTIN	22) SIGTTOU	23) SIGURG	24) SIGXCPU	25) SIGXFSZ
26) SIGVTALRM	27) SIGPROF	28) SIGWINCH	29) SIGIO	30) SIGPWR
31) SIGSYS	34) SIGRTMIN	35) SIGRTMIN+1	36) SIGRTMIN+2	37) SIGRTMIN+3
38) SIGRTMIN+4	39) SIGRTMIN+5	40) SIGRTMIN+6	41) SIGRTMIN+7	42) SIGRTMIN+8
43) SIGRTMIN+9	44) SIGRTMIN+10	45) SIGRTMIN+11	46) SIGRTMIN+12	47) SIGRTMIN+13
48) SIGRTMIN+14	49) SIGRTMIN+15	50) SIGRTMAX-14	51) SIGRTMAX-13	52) SIGRTMAX-12
53) SIGRTMAX-11	54) SIGRTMAX-10	55) SIGRTMAX-9	56) SIGRTMAX-8	57) SIGRTMAX-7
58) SIGRTMAX-6	59) SIGRTMAX-5	60) SIGRTMAX-4	61) SIGRTMAX-3	62) SIGRTMAX-2
63) SIGRTMAX-1	64) SIGRTMAX	
-------------------------------------------
whirr with (EMR on M7)
export AWS_ACCESS_KEY_ID=AKIAI2WV3RWOCB3ONHOQ
export AWS_SECRET_ACCESS_KEY=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
-------------------------------------------
yum uninstall package   yum -e <pkg_name> centOS rpm red hat
yum erase <pkg_name>   on some machines . check help options for correct usage
yum install packagename

if got package not signed error below should work
rpm -ivh packagename
-------------------------------
synchronize your unix system time thru internet in centOS
yum install ntp
chkconfig ntpd on
ntpdate pool.ntp.org
/etc/init.d/ntpd start
hwclock --systohc
date

change time zone in linux
ln -sf /usr/share/zoneinfo/America/Los_Angeles /etc/localtime
-----------------------------------
if /etc/yum.repos.d/mapr.repo  is not updating correctly, purge the cache and reinstall
rm -rf /var/cache/yum
yum clean headers
yum clean packages
yum clean metadata
yum clean all
-------------------------------------
add these in /etc/apt/sources.list
deb http://apt.qa.lab/mapr mapr optional
deb http://apt.qa.lab/opensource binary/
----------------------------------------
tar -cvf suhas.tar  hadoop-hdfs-httpfs-0.20.2-cdh3u6/
------------------------------------------
hue python.h dependency 

 yum install python-devel
 will have ls /usr/include/python2.6/Python.h
----------------------------------
git commit failed 

Permission denied (publickey).
fatal: The remote end hung up unexpectedly

test permissions with  ssh -T git@github.com
follow steps here - https://help.github.com/articles/generating-ssh-keys
--------------------------------------------------
lsof -i:10000
to check if anything is listening on port 10000

netstat -tulpn | grep 21180
shows all the ports (tcp, ava, udp, etc) that process with pid 21180 is listening on (after opening them)
--------------------------------------------------
running OOzie commands
export OOZIE_URL="http://10.10.30.153:11000/oozie" as an
on secure oozie cluster, set fqdn for OOZIE_URL - http://perfnode153.perf.lab:11000/oozie

environment variable
bin/oozied.sh start
bin/oozied.sh stop
bin/oozie admin -status
bin/oozie job -config job.properties -run
ps -axu | grep 0000016-130826142717853-oozie-root-W

bin/oozie job -kill <job-id>
./bin/oozie job -config examples/apps/map-reduce/job.properties -run
./bin/oozie job -info 0000005-130916182438945-oozie-mapr-W

service -oozie start (will always start the process as user mapr) but starting oozied.sh script launches it as unix.user

starting oozie with maprcli
maprcli node services -name oozie -action stop -nodes 10.10.10.151

maprcli node services -name hbasethrift -action start -nodes node001

if oozie pig job launched from hue runs out of memory with java heap spac in jobtracker logs seen thru mcs, 
increase mapper heap size  in oozie workflow.xml with this property- 
<property>
   <name>oozie.launcher.mapred.child.java.opts</name>
   <value>-Xmx2048m</value>
</property>

oozie error codes and what they mean - 
Automatic retry interval for workflow action is handled for these specified error code: 
FS009, FS008 is file exists error when using chmod in fs action. 
JA018 is output directory exists error in workflow map-reduce action. 
JA019 is error while executing distcp action. 
JA017 is job not exists error in action executor. 
JA008 is FileNotFoundException in action executor. 
JA009 is IOException in action executor.
-------------------------------------------

maprcli node list -columns "services,configuredservice"   
-to check the status of configured services thru warden

or 
maprcli dahbregionserverfo

to start hue from warden - 
maprcli node services -name <servicename> -action <start/stop/restart> -node <list of nodes>
maprcli node services -nodes 10.10.100.109 -name hue -action stop
maprcli node services -name hue -action restart -nodes 10.10.10.152

maprcli node services -nodes  10.10.30.152 mfs start
starts mfs

restart jobtracker
maprcli node services -nodes 10.10.30.152  -jobtracker restart
maprcli node services -nodes 10.10.30.152  -hbmaster restart
maprcli node services -nodes 10.10.30.152  -hbregionserver restart


restart tasktracker
../bin/hadoop-daemon.sh  --config ../conf/  start tasktracker

create volumes cmd -
maprcli volume create -name jontest -path /jontest

observation: tasktracker shutting down with logs msg - reason:  can't create any volumes, not just the local volume for TT.

2014-05-13 11:09:53,074 INFO org.apache.hadoop.mapred.TaskTracker: Checking for local volume. If volume is not present command will create and mount it. Command invoked is : /opt/mapr/server/createTTVolume.sh 10.10.30.152 /var/mapr/local/10.10.30.152/mapred/ /var/mapr/local/10.10.30.152/mapred/taskTracker/ 
2014-05-13 11:10:11,408 ERROR org.apache.hadoop.mapred.TaskTracker: Failed to create and mount local mapreduce volume at /var/mapr/local/10.10.30.152/mapred/. Please see logs at /opt/mapr/logs/createTTVolume.log
2014-05-13 11:10:11,408 ERROR org.apache.hadoop.mapred.TaskTracker: Command ran /opt/mapr/server/createTTVolume.sh 10.10.30.152 /var/mapr/local/10.10.30.152/mapred/ /var/mapr/local/10.10.30.152/mapred/taskTracker/ 

from: /opt/mapr/logs/createTTVolume.500.log
2014-05-13 11:30:44,7352 ERROR JniCommon fs/client/fileclient/cc/jni_common.cc:1407 Thread: 139702587934464 GetUserInfo: Unable to get the IsImpersonatedUser method ID of the MapRUserInfo object from Java

solution - try reducing mfs memory % from 35% to 20%
--------------------------------------------------
local install of maven artifacts - local build of hadoop-core.jar with mercurial hg "make hadoop" in ~/dev/trunk/src branch to local repo - 
mvn install:install-file -Dfile=~/test/test_hue/hadoop_jars/subhash_hadoop_StringUtils_fix/hadoop-core-3.0.2.jar  -DgroupId=org.apache.hadoop -DartifactId=hadoop-core -Dversion=3.0.2 -Dpackaging=jar

source: http://maven.apache.org/guides/mini/guide-3rd-party-jars-local.html
--------------------------------------------------
warden restart cmd 
/etc/init.d/mapr-warden start
/etc/init.d/mapr-warden stop
/etc/init.d/mapr-warden status
-------------------------------
rpm -qa | grep "hive"
yum list installed | grep ssl
-------------------------------
rpm -qpR {.rpm-file}
tells dependencies of an RPM file. 

rpm2cpio rpm_pkg_name | cpio 

prints  dependencies of an rpm file on stdout- 
rpm -ivh hue-common-3.5.0+cdh5.0.2+373-1.cdh5.0.2.p0.14.sles11.x86_64.rpm 

-------------------------------
login as root
sudo su <ssatish pwd>

change unix user password as root - 
passwd ssatish
>new password
-------------------------------
/var/lib/dpkg/info/xml-core.{pre,post}{inst,rm} 
pre-install post-install scripts for .deb pkg

without installing deb package , you can do 
ar -x hue-common_2.5.0+139-1.cdh4.4.0.p0.70~squeeze-cdh4.4.0_amd64.deb
this creates a control.tar.gz and if you extract it, you will see the dependencies in control
and preinst and postinst scripts.
-----------------------------------------------------------------
top 
gives amount of memory consumed - shift-M sorts by descending amount of resident memory used. Chrome was having a memory leak (known issue on the internet with ubuntu 12.04 LTS). Restarting it solved the problem.
-------------------------------------------
flume build command 
mvn compile install -DskipTests -Drat.numUnapprovedLicenses=100
-------------------------------------------
oozie build command - 
bin/mkdistro.sh -DskipTests 
--------------------------------
for staging files after git add to check diff

backing out the most recent changes - 
git revert HEAD
git push

git revert -m 1 (Commit id of the merge commit)
git push 

backing out some other change not at the top of the list
git revert dd61ab32
git push

backout git committed (or reverted) but not pushed - 
git reset --hard origin/master

search a git commit by commit ID 
git log -g | grep -A 5 -B 5 a6ca15a1466bd38ef5aa73a90d80d70ea0cfbf41

git log --author="vpeesapati\|Venkata Surya Gowtham Peesapati" > goutham_commits.log


then do 
git lg
------------
checkout a particular git tag - 
git checkout tags/cdh5.0.0-release
git checkout tags/v7-02-release

git tag -a v7-04-release
git push --tags (pushes tag to remote branch)
-------------------------------
hive -e "set" | grep metastore
check the default values of properties that are set  in hive
-------------------------------------
core-site.xml on every task tracker node - 
fs.mapr.trace=debug
to enable file client debugging

to do the same on client side from pig grunt shell for example, 
set fs.mapr.trace 'debug';


 maprcli trace setlevel -level debug
------------------------------------------------
 jps -v
prints the java process arguments

if 'jps' command says 'process information unavailable', 
following may fix it. permissions may have got messed up to 777 below. fix it as -
chmod -R 755 /tmp/hsperfdata_* 

unix sticky bit - 
chmod 7777 file_name
gives rwsrwsrwt

--------------------------------------
hue sqlite user 

sqlite3 /opt/mapr/hue/hue*/desktop/desktop.db
sqlite> select username from auth_user;
sqlite> select * from auth_user;
sqlite> delete from auth_user;


display table schema
sqlite> .schema auth_user

cant be null values if NOT NULL mentioned in schema
insert into auth_user (id, username, password) values (100, 'demo_user', 'mapr');

insert into auth_user (id, username, password, first_name, last_name, email, is_staff, is_active, is_superuser, last_login, date_joined) values (1, 'mapr', 'mapr', 'mapr', 'mapr', 'ssatish@maprtech.com', 'true', 'true', 'true', DATETIME('now', 'localtime'), DATETIME('now', 'localtime'));


you can also create user from Hue shell app - 
./build/env/bin/hue shell
>>> from django.contrib.auth.models import User
>>> User.objects.create(id=1, username='mapr')
>>> user = User.objects.get(username='mapr')
>>> user.set_password('mapr')
>>> user.is_superuser = True
>>> user.save()



User.objects.all().delete()

./build/env/bin/hue dbshell
sqlite>  select * from useradmin_huepermission;

sqlite> .tables
========================================
HUE Fundaes - thumb rules and usage tips
========================================
Beeswax keeps many of its queries open, which will consume memory. This may cause beeswax service to crash with outofMemory exception even when heap size is ~20GB. Switch to hive 

build/env/bin/hue close_queries 7 all
https://github.com/cloudera/hue/blob/master/apps/beeswax/src/beeswax/management/commands/close_queries.py#L26
DESKTOP_DEBUG=1 build/env/bin/hue runcpserver

--------------------------------------
debug httpfs error - hue config validation error -
Failed to create temporary file hue_config_validation
do this below for debug - 
curl -i "http://10.10.30.152:14000/webhdfs/v1/?op=LISTSTATUS&user.name=mapr&doas=ssatish"

Under the cover Hue is actually doing this:
curl -i -X PUT "
http://localhost:50070/webhdfs/v1/tmp/test_hue?op=LISTSTATUS&user.name=hue&doas=hdfs
"
We first restarted Hadoop, and the misconfiguration error disappeared.

curl "http://localhost:14000/webhdfs/v1/user/mapr/blah.txt?op=open&user.name=mapr"
build httpfs package - 
mvn package -Pdist -DskipTests

testing https over httpfs (ssl) with self-signed certificates - 
curl -k "https://localhost:14000/webhdfs/v1/user/mapr/blah.txt?op=open&user.name=mapr"

curl -ik -X PUT --user suhas:mapr "https://localhost:14000/webhdfs/v1/user/ssatish/blah_dir3?op=mkdirs&user.name=ssatish"

impersonation - 
curl -ik -X PUT "https://localhost:14000/webhdfs/v1/user/ssatish/blah_impersonate?op=mkdirs&user.name=mapr&doas=ssatish"

secure httpfs working correctly - creation of file- 
curl --negotiate -u : -b ~/cookiejar.txt -c ~/cookiejar.txt -i -X PUT "http://perfnode153.perf.lab:14000/webhdfs/v1/user/mapr/blah7?op=MKDIRS"

curl "https://s3.amazonaws.com/aws-cli/awscli-bundle.zip" -o "awscli-bundle.zip"
---------
httpfs supported operations - 
./src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java
./src/main/java/org/apache/hadoop/fs/http/client/HttpFSFileSystem.java
    OPEN(HTTP_GET), GETFILESTATUS(HTTP_GET), LISTSTATUS(HTTP_GET),
    GETHOMEDIRECTORY(HTTP_GET), GETCONTENTSUMMARY(HTTP_GET),
    GETFILECHECKSUM(HTTP_GET),  GETFILEBLOCKLOCATIONS(HTTP_GET),
    INSTRUMENTATION(HTTP_GET),
    APPEND(HTTP_POST),
    CREATE(HTTP_PUT), MKDIRS(HTTP_PUT), RENAME(HTTP_PUT), SETOWNER(HTTP_PUT),
    SETPERMISSION(HTTP_PUT), SETREPLICATION(HTTP_PUT), SETTIMES(HTTP_PUT),
    DELETE(HTTP_DELETE);

----------------------------------------------------------------------------
to see on rpm/centOS machine if postfix server is automatically started on linux bootup - if linux boots into runlevels 2,3,4,5 then postfix service will start automatically 

/sbin/chkconfig --list | grep postfix
postfix        	0:off	1:off	2:on	3:on	4:on	5:on	6:off

to change auto-start-up on bootup 
/sbin/chkconfig --list | grep sendmail
--------------------------------------------------------
sshpass -p "password" scp ssatish@10.250.0.77:/home/ssatish/git_repos/hue-2.5.0-mapr/apps/pig/src/pig/views.py apps/pig/src/pig/views.py

passwordless ssh
ssh-keygen
cat ~/.ssh/id_rsa.pub | ssh s6 'cat >> ~/.ssh/authorized_keys'

set up passwordless ssh for user root from 10.10.30.84 to 10.10.30.152
on 30.84 as user root:
ssh-keygen
ssh-copy-id root@10.10.30.152

then do rolling upgrade of cluster after installing "mapr-upgrade"  pkg on all cluster nodes
no need to stop warden or zookeeper.
command - 
./rollingupgrade.sh -s -v 3.0.3

/opt/mapr/logs/singlenodeupgrade.log
/opt/mapr/logs/singlenodeupgrade.log.summary
---------------------------------
from shell script, multi line input to db shell - 
cat << EOF | build/env/bin/hue shell
from django.contrib.auth.models import User
User.objects.create(id=1, username='root') 
user = User.objects.get(username='root')
user.set_password('mapr')
EOF
-------------------------------------
test build on build machine - 
/root/builds/opensource/node/10.10.1.77
make mapr-hue-clean
make mapr-hue
-----------------------------------------
hue uses ipython from 
/home/suhash/git_repos/hue-2.5.0-mapr/build/env/lib/python2.7/site-packages/ipython-0.10-py2.7.egg/IPython/UserConfig

`logname`  is used to get acual linux proxy user who launched command or shell script. 
----------------------------------------------------------------------------------------
~/.config/google-chrome/Default/Bookmarks
--------------------------------------------
starting hbase client servies like hbase rest and thrift servers - 
/bin/hbase-daemon.sh start thrift
/bin/hbase-daemon.sh stop thrift
logs - 
/opt/mapr/hbase/hbase-0.94.13/bin/../logs/hbase-mapr-thrift-perfnode152.perf.lab.out
-------------------------------------------
kerberos configuration for hbase
kadmin -p mapr/admin
	:listprincs
	:addprinc	mapr/<clustername>
	:ktadd -k /opt/mapr/conf/mapr.keytab mapr/<clustername>

klist -k /opt/mapr/hue/hue-2.5.0/desktop/conf/hue.keytab
lists keys in a kerberos keytab file
------------------------------------------------------------------
getting kerberos ticket from keytab file for a specific pricipal
kinit -k -t hue.keytab mapr/perfnode153.perf.lab@QA.LAB

if kinit throws error - 
kinit: Password incorrect while getting initial credentials
the reason you can't authenticate with a password is because you created the keytab.

The act of creating a keytab (using ktadd -k mapr.keytab cmd above) causes a new random key to be generated
and placed in the Kerberos database and into the keytab (/opt/mapr/conf/mapr.keytab). There is no
password associated with that key and you will only be able to
authenticate as that principal using the keytab (that keytab in this case is not readable by ssatish user). 

If you want to authenticate with a password, do a "cpw" in kadmin for
the principal (and do not do a "ktadd").
SOLVED: this works -  without doing ktadd, do 
kinit ssatish/perfnode181.perf.lab@dev-maprtech

kerberos error on hadoop fs -ls / -> 
>>> unsupported key type found the default TGT: 18
solved by using the correct java JCE policy files downloaded from oracle, unzipped and put the jars under
$JAVA_HOME/jre/lib/security/
------------------------------------------------------------------
env variable  to cache kerberos credentials
export KRB5CCNAME="/tmp/hue_krb5_ccache"
kinit -k -t /opt/mapr/conf/cluster.keytab -l 365d -c /tmp/hue_krb5_ccache mapr/perfnode181.perf.lab@dev-maprtech

undo bash export env variable - 
unset KRB5CCNAME
-------------------------------------------------------------------------
max renewable lifetime of a kinit'd ticket and ticket lifetime are controlled by the followig files-
1) /etc/krb5kdc/kdc.conf or /var/kerberos/krb5kdc/kdc.conf
2) /etc/krb5.conf
(kdc is running on 10.10.100.133  kadmin -p mapr/admin 
pwd: mapr123! )

1) takes precedence and can override  2) 

kdc.conf - 
max_life = 7d 0h 0m 0s
max_renewable_life = 365d 0h 0m 0s

takes percedence over krb5.conf - 
ticket_lifetime = 7d
renew_lifetime = 365d

If you plan to use Oozie, Impala, or the Hue Kerberos ticket renewer in
your cluster, you must configure your KDC to allow tickets to be renewed,
and you must configure krb5.conf to request renewable tickets. Typically,
you can do this by adding the max_renewable_life setting to your realm in
kdc.conf, and by adding the renew_lifetime parameter to the
libdefaultssection of
krb5.conf. For more information about renewable tickets, see the Kerberos
documentation <http://web.mit.edu/Kerberos/krb5-1.8/>.
http://grokbase.com/t/cloudera/hue-user/144e7xrjnc/what-should-i-do-when-renew-kerberos-ticket-for-hue

(cdh4.5.4 docs) -
Oozie and Hue require that the realm support renewable tickets.
http://www.cloudera.com/content/cloudera-content/cloudera-docs/CM4Ent/4.5.4/Configuring-Hadoop-Security-with-Cloudera-Manager/Configuring-Hadoop-Security-with-Cloudera-Manager.html

--------------------------------------------------------------
important kerberos rules - 
1. When you change your Kerberos password, you will need to recreate all your keytabs. 

2. If its a server, (Hue is server to its clients), then you need to have a kerberos keytab file for the server principal. hence you specify hue.keytab with a principal name for the hue clients to connect to.

3. If its a client (also a kerberos client in the case where hue tries to talk to jobtracker), hue needs to have server (JT principal) and needs to have a kinit'd ticket for the server principal. 

4. /opt/mapr/conf/cluster.keytab has infinite lifetime and service daemons (JT, TT, file server,etc) do not need to have a kinit'd ticket if using this keytab. 

5. when we regenerate keytab file, kvno will auto increase (keytab and kdc).

6. kinit -R renews ticket. Expired tickets cannot be renewed even if they're within their renewable time window. 

7. troubleshooting guide - add these to the jvm -
-Dsun.security.krb5.debug=true -Dsun.security.spnego.debug=true -Djavax.net.debug=all

should i run 
configure.sh -K  or -kerberosEnable ?

8) whats happening below the hood when you do maprlogin is that a https connection is opened and java client (maprlogin utility) 
is sending a kerberos spnego token to the cldb.

9) cldb validates the token using java kerberos GSSAPI (generic security services application program interface) 
and returns a mapR token- the same token is also returned from userid and password authentication
---------------------------------------------------
start nfs service
maprcli node services -nodes 10.10.30.153 -nfs start
showmount -e 10.10.30.153
----------------------------
mkdir /mnt/license
mount 10.250.1.5:/home/mapr-data/license /mnt/license
mount 10.10.30.153:/mapr /mnt/mapr
---------------------------
mount   #returns below on 10.10.30.153 because nfs gateway is mounted as part of autofs
10.10.10.20:/mapr/selfhosting on /home/MAPRTECH type nfs (rw,addr=10.10.10.20)
10.250.1.5:/home/mapr-data/license on /mnt/license type nfs (rw,addr=10.250.1.5)
--------------------------------------------------
maprcli acl edit -type cluster -user username:fc

command to see which services are configured to run on which cluster nodes - 
maprcli node list -columns hostname,service,configuredservice


maprcli cluster mapreduce get
gets MR1 (classic) or MR2 (yarn)
can also be changed thru MCS


m7 license - 
copy license from mac -
/Users/ssatish/selfhosting/license
maprcli license add -license ./LatestV3DemoLicense.txt -is_file true
----------------------------------------------
hostname -f 
gives the fully qualified domain name (FQDN) 

ping perfnode153.perf.lab
to check if machine responds 

HOSTNAME of linux desktop 
ssatish-NP192AA-ABA-p6140f

hostname -f
hostname: Host name lookup failure

this error is because dns lookup fails. 

correct DNS name resolution in 
/etc/resolv.conf
by setting - 
nameserver 10.10.1.10
--------------
1440x900
default screen resolution in mac os x el capitan

apple symbol -> system preferences -> displays -> scaled -> hover mouse on default image to get screen resolution. 
this is important for gif images import into power point without blurring

most of the projectors also have a max resolution of 1024 × 768
they spread them out over  alarger surface area 

save your png with the same resolution as the display resolution for presentations. 
-------------
correct hostname permanently by setting it in - 
/etc/sysconfig/network

reboot machine after these changes. 
mapr cluster has to be completely shut down and restarted

otherwise, atleast 2 adverse effects that are possibly observed are - 
tasktraker will create new local volumes. zk data will be different, it has to be cleaned with zk_cleanup.sh (which will not erase cldb data since its on cid 1 at epoch 4)

zookeeper servers should be specified in the same order as initially to the mapr-installer

to use mapr-installer on redhat/centOS machines, install epel repo first-
http://www.rackspace.com/knowledge_center/article/installing-rhel-epel-repo-on-centos-5x-or-6x
wget http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm
yum install ./epel-release-6-8.noarch.rpm
yum install sshpass
(mapr-installer wont run without sshpass installed)
---------------------------------------------
oozie kerberos principal name is specified in mapr.login.conf as HTTP/perfnode153.perf.lab
---------------------------------------------
put ivy resolve dependencies list (new jar files) in ivy.xml as in the case of Pig
-----------------------------------------
maven dependency tree excludes - 
mvn dependency:tree
-------------------------------
look at ivy.xml for any modifications required to ivy download of maven artifacts. for example, pig had to add dependency on jsp (reference to opt/mapr/hadoop would fail on build machine as theres no guarantee of existence of that path, so pig needs to bundle n package it within it) for piggybank to ivy.xml and librarries.properties to resolve build issues with this check-in 

only hive-10 and pig-10 were released as package com.mapr.hive and com.mapr.pig
hive9,11,12 and pig11,12  were released as org.apache.hive and org.apache.pig
--------------------------------------------------------
pig build - 
----------
cd lib
wget http://www.eli.sdsu.edu/java-SDSU/sdsuLibJKD12.jar
zip -d sdsuLibJKD12.jar  'junit/*'
cd ..
ant jar-all


generate data and run pig unit pre-commit end2end tests
sudo ant -Dharness.old.pig=/home/ssatish/git_repos/pig-0.11-mapr -Dharness.cluster.conf=/opt/mapr/hadoop/hadoop-0.20.2/conf -Dharness.cluster.bin=/usr/bin/hadoop -Dharness.hadoop.home=/opt/mapr/hadoop/hadoop-0.20.2 test-e2e-deploy

 tests can be run faster by  using the parameters -Dfork.factor.conf.file=5 -Dfork.factor.group=10. On a 10 node cluster it should execute in 2 hours.
-------------------------------------------------
flume build - 
mvn compile install -DskipTests -Drat.numUnapprovedLicenses=100
------------------------------------------------------------------
nohup - terminal shell command to  run a command immune to hangups, with output to a non-tty

unix linux shell terminal colouring -enable by putting this in .bashrc

alias less='less --RAW-CONTROL-CHARS'
export LS_OPTS='--color=auto'
alias ls='ls ${LS_OPTS}'
-----------------------------------------
launch intelliJ idea IDE - alternative to eclipse - 
/home/ssatish/Desktop/idea-IC-129.1359/bin/idea.sh

intellij keyboard shortcuts - 
/home/ssatish/MapR_docs/intellij_keyboard_shortcuts.txt

mac short cut in intellij for changing project SDK - 
cmd ;
---
cmd + alt +B - show definition of a method (search for implementations)
cmd + y - show definition of an API method in a pop-up
cmd + fn +F12 - quick scan the methods of a class
cmd + [ - jump to previous cursor/file position
cmd + ] - jump to next cursor position
-------------------------------------------
java inheritance - 
consider 2 classes  AisP and BisC where AisP is parent class and BisC is inherited child class 

JobConf is a class inherited from Configuration. 
Declared method in mapr.fs is  getJobTrackerAddrs(Configuration conf)
called method in pig thru reflection is getJobTrackerAddrs(JobConf jobconf)

Inheritance: is a 
Delegation (Interface): has a 
Apple is a Fruit

java reflection example - 
                    FileSystem fs = FileSystem.get(jobConf);
                    Class<? extends FileSystem> fclass = fs.getClass();
                    Method method = fclass.getMethod("getJobTrackerAddrs", new Class[]{Configuration.class});
                    InetSocketAddress[] jobTrackerAddr = (InetSocketAddress[]) method.invoke(fs, jobConf);

reflection can slow down code. 
similarly, exception creation is not runtime efficient. it is better to avoid creating exceptions whenever possible. Try to return null and log it with log.info but that may not always be  possible if the function is flat and very long. Then initialize it to null and have null checks. 

 
---------------------------------------------------------------
Yuliya hadoop map-reduce hive experienced person - interview question - 
Implement Writable interface of hadoop.
-------------------------------------------
resolving DNS hosts by putting in /etc/hosts the following line - 
10.10.30.84     perfnode84.perf.lab

now urls get resolved by hostname without needing IP@
--------------------------------------------
To enforce strict checking of output location, set "pig.location.check.strict=true". By default, it is false.

+        System.out.println("    -printCmdDebug - Overrides anything else and prints the actual command used to run Pig, including");
+        System.out.println("                     any environment variables that are set by the pig command.");
--------------------------------------------------------------
/opt/mapr/bin/sqoop import -connect jdbc:mysql://10.10.1.10/TestDB --username mapr --password mapr  --table t1 --hbase-table t1.${x} --column-family imp --hbase-create-table

/opt/mapr/bin/sqoop import -connect jdbc:mysql://10.10.1.10/TestDB --username mapr --password mapr  --table t1 --hive-import

/opt/mapr/sqoop/sqoop-1.4.4/bin/sqoop import -D hadoop.job.history.user.location=none -connect jdbc:mysql://10.10.1.10/TestDB --username mapr --password mapr  --table t1 --hive-import

mysql server commands - 
mysql server running on 10.10.100.56. ssh into the m/c and start mysql prompt with - 

mysql -uroot -pmapr
mysql -u root -h 10.10.100.56 

if above throws following error, try cmd below
ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/var/lib/mysql/mysql.sock' 
mysqladmin -u root -p status
restart the server with - 
/sbin/service mysqld start

to view databases on the mysql server - 
mysql> show databases;

to select a database called test- 
mysql> use test;

to show tables on DB test -
mysql> show tables;
+----------------+
| Tables_in_test |
+----------------+
| student        |
| voter          |
+----------------+

voter schema-
|   1000 | wendy falkner           |   49 | republican   |         366.7 |     28540 |

student schema - 
|   1000 | ulysses young       |   45 | 3.51 | 800020115821 |

to exit - 
mysql> quit;

ERROR 1045 (28000): Access denied for user 'root'@'10.250.0.77' (using password: YES)
solved by logging into mysql AFTER sshing into remove machine where its running -
mysql> grant all privileges on *.* to ''@'10.250.0.77' with grant option;
mysql> select User, Host, Password from mysql.user;

to put a password - 
> grant all privileges on *.* to 'root'@'10.250.0.77' identified by 'mapr' with grant option;
> flush privileges;

mysql -u root -p -h 10.10.100.56
password: mapr

show mysql table schema - 
show columns from voter;

----------------------------------------------------------------------------------------

connecting from hue sqoop2 shell - 

jdbc driver class - com.mysql.jdbc.Driver

jdbc connection string -  jdbc:mysql://10.10.100.56/test

Username: root
password: mapr

JDBC Connection Properties:


--------------------------------------------
reference - 
http://archive.cloudera.com/cdh4/cdh/4/sqoop2/CommandLineClient.html


./bin/sqoop.sh client   (starts sqoop shell)

sqoop> set server --host localhost --port 8080 --webapp sqoop

sqoop> start job --jid 6   
-----------------------------------------------------------
sqoop2 git checkout and build - 

git clone https://git-wip-us.apache.org/repos/asf/sqoop.git sqoop2
git checkout sqoop2

latest docs -
http://sqoop.apache.org/docs/1.99.3/BuildingSqoop2.html
http://sqoop.apache.org/docs/1.99.3/Installation.html

mvn install -Dhadoop.profile=100 -DskipTests   (target compile is broken)
mvn package -Pbinary -Dhadoop.profile=100 -DskipTests -Drat.numUnapprovedLicenses=100

----------------------------------------
start metastore as a service before starting  hiveserver2
/opt/mapr/hive/hive-0.12/bin/hive --service metastore &
/opt/mapr/hive/hive-0.12/bin/hive --service hiveserver2 &
./hive --start --service metastore

checking from beeline - 
http://doc.mapr.com/display/MapR/Using+HiveServer2
/opt/mapr/hive/hive-0.12/bin/hive --service beeline
!connect jdbc:hive2://localhost:10000
Connecting to jdbc:hive2://localhost:10000

Enter username for jdbc:hive2://localhost:10000: root
Enter password for jdbc:hive2://localhost:10000: ********
Connected to: Hive (version 0.12-mapr-1401)
Driver: Hive (version 0.12-mapr-1401)
Transaction isolation: TRANSACTION_REPEATABLE_READ
0: jdbc:hive2://localhost:10000> CREATE TABLE pokes (foo INT, bar STRING);
No rows affected (1.301 seconds)
0: jdbc:hive2://localhost:10000> show tables;
+-----------+
| tab_name  |
+-----------+
| pokes     |
+-----------+
1 row selected (0.261 seconds)
--------------------------------------

secure cluster with kerberos secured hiveserver2 (even non-mapr user like ssatish uses mapr server principal while connecting) - (after having kinit'd ticket for that user)

 !connect jdbc:hive2://10.10.30.181:10000/default;principal=mapr/perfnode181.perf.lab@dev-maprtech
above works

below doesnt work on hive-0.13 , asked how to use on jira  - 

secure cluster beeline connect to hiveserver2 secured with ssl
*********************
hive-0.13
!connect jdbc:hive2://10.10.30.153:10000/default;ssl=true;sslTrustStore=/opt/mapr/conf/ssl_truststore;trustStorePassword=mapr123
*********************
working cmd for hive-0.12
!connect jdbc:hive2://10.10.100.115:10000/default;ssl=true;sslTrustStore=/opt/mapr/conf/ssl_truststore;sslTrustStorePassword=mapr123
*******************sl
note: username and password are prompted from beeline with ssl commands abov, but are ignored. 



testing hiveserver2 <-> kerberos <-> hive-metastore security channel - 

try starting hive metastore without security with cmd 
 ./bin/hive --service metastore --hiveconf hive.server2.authentication=""

then start hiveserver2 in secure mode with hive-site.xml having - 
hive.server2.authentication=kerberos
this ensures that if 1 side is not secured, some exceptions will be throws.

---------------
creating ssl keystore and truststore to use with hive-0.11  
http://answers.mapr.com/questions/7979/hiveserver2-jdbc-ssl-not-working

Generate keystore file. Keytool will ask for a password. 
   keytool -genkeypair -alias certificatekey -keyalg RSA -validity 7 -keystore keystore-file-name

Generate certificate from keystore 
   keytool -export -alias certificatekey -keystore keystore-file-name -rfc -file certficate-file-name

From the certificate generate truststore 
   keytool -import -alias certificatekey -file certificate-file-name -keystore truststore-file-name

Use the keystore file and keystore password at HiveServer2 hive-site.xml

On client side you can either use the certificate (generated in step 2) or truststore (generated in step 3). (Instructions are here)
------------------
lists the aliases (keys) of certificates contained in the keystore-
keytool -list -keystore /opt/mapr/conf/ssl_keystore 
output:
Enter keystore password:  
Keystore type: JKS
Keystore provider: SUN
Your keystore contains 1 entry
my.cluster.com, Jul 1, 2014, PrivateKeyEntry, 
Certificate fingerprint (MD5): 39:5C:DF:BD:1E:61:57:B9:53:BA:EF:6E:6C:6B:4C:6D

export certificate from keystore file(tested) - 
keytool -exportcert  -alias my.cluster.com -keystore /opt/mapr/conf/ssl_keystore -rfc -file httpfs.cert

import this certificate into the default java keystore - 
keytool -import -alias my.cluster.com -file httpfs.cert -keystore /usr/java/jdk1.6.0_45/jre/lib/security/cacerts
The default password is changeit.
http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/CDH5-Security-Guide/cdh5sg_httpfs_ssl.html

-------------------------
SSL self-signed certificate generation (untested) - 
keytool -genKey -keyalg RSA -alias selfsigned -keystore /opt/mapr/conf/ssl_keystore -validity 360 -keysize 2048
this will create a keystore.jks file (ssl_keystore) containing a private key and your sparkingly fresh self signed certificate *.csr

signed certificate from certification authority (untested)- 
keytool -certreq -file SuhasS.csr

 -----------------------
0: jdbc:hive2://localhost:10000> LOAD DATA LOCAL INPATH './examples/files/kv1.txt' OVERWRITE INTO TABLE pokes_ssatish_beeline;
Error: Error while compiling statement: FAILED: RuntimeException java.io.IOException: Error getting user info for current user, anonymous (state=42000,code=40000)

--------------------------------
 to export sql table - 
 mysqldump -p --user=root test voter > voter.sql

to import .sql file as a table into mysql - 
mysql -u username -p -D dbname < tableName.sql

create table voterimport (rownum int(11) default NULL, name varchar(50) default NULL, age tinyint(4) default NULL, registration varchar(15) default NULL, contributions float default NULL, voterzone smallint(6) default NULL);

before sqoop export, table name should already exist in mysql. 


logs under 
/tmp/root/hive.log

table created as - 
drwxr-xr-x   - root root          0 2014-01-24 18:20 /user/hive/warehouse/pokes

beeline username/password is PAM authenticated even for non-secure cluster 
drwxr-xr-x   - mapr mapr          0 2014-01-24 18:24 /user/hive/warehouse/pokes_mapr

customize location of hive table date -
<property>
   <name>hive.metastore.warehouse.dir</name>
   <value>/user/hivestore/warehouse </value>
   <description>location of the warehouse directory</description>
 </property>


describe extended <hive_table_name>
-above cmd gives detailed table schema, field schema and input/output paths from which hue's hive_server2_lib.py gets info for displaying in logs. 
==================================================================
gnome-open ./src/main/javadoc/org/apache/hadoop/hbase/thrift/package.html
opens a html page from any java docs in default browser from ubuntu cmd-line shell
===================================================================
 Homebrew is “The missing package manager for apple mac OS X”
===================================================================

 to avoid downtime for upgrading cloudera packages (rolling upgrade), they are done by cloudera manager thru 'parcels' which have 6 phases - parcels are managed in six steps: Download, Distribution, Activation, Deactivation, Removal, and Deletion.
AbE - aprcel is just a .tar.gz file
-------------------------------------------------
building spark from git checkout - 
like make clean build
./sbt/sbt clean assembly
building with sbt may not shade patterns from pom.xml properly, leading to runtime failure to launch spark-shell
*****************
build with maven - 
mvn -Dhadoop.version=1.2.1 -DskipTests clean package
*****************


hint: cloudera foesnt have a git fork to spark yet, so no cdh specific changes in it. spark oozie app currently, but shifting to spark job server from ooyala in future. 
===================================================================
hive build and unit tests 
before running unit tests increase ant jvm memory with - 
export ANT_OPTS='-Xms1536m -Xmx1536m -XX:PermSize=4096m -XX:MaxPermSize=4096m'
else you get hcatalog/build-support/ant/deploy.xml:81: java.lang.OutOfMemoryError: PermGen space
after 903 minutes
but with that xmx value, Error occurred during initialization of VM
Could not reserve enough space for object heap


compile and run unit tests with (change maven/)
**************************
ant clean package tar test
**************************
generate results on html page with 
ant testreport
[junitreport] Processing /home/suhash/git_repos/hive-0.12-4629.2/build/test/TESTS-TestSuites.xml to /home/suhash/git_repos/hive-0.12-4629.2/build/test/junit-noframes.html

hive-0.13 - maven surefire plugin - grep *surefire* - html pages
==================================================================
how to setup umask in linux - (root has typical value 022 and other users have 002)
http://www.cyberciti.biz/tips/understanding-linux-unix-umask-value-usage.html
files  (666 minus umask) = actual permission = 666 - 022 = 644 (user, group, other)
directories  (777 minus umask) = actual permission = 777 - 022 = 755  
===================================================================
install oracle sun java 6 on ubunutu 12.04 - 
http://linuxg.net/how-to-install-oracle-java-jdk-678-on-ubuntu-13-04-12-10-12-04/

install oracle sun java 7 on ubuntu 12.04 -

http://www.webupd8.org/2012/01/install-oracle-java-jdk-7-in-ubuntu-via.html

apt-get install python-software-properties
add-apt-repository ppa:webupd8team/java
apt-get update
apt-get install oracle-java7-installer
java -version
echo $JAVA_HOME  (change in ~/.bashrc)
update-java-alternatives -s java-7-oracle

on mac, adding this to bashrc will set JAVA_HOME to proper oracle java version 
export JAVA_HOME=`/usr/libexec/java_home -v 1.6`

rhel5 rhel6 centOS5 centOS6 - oracle sun java 1.7 jdk install -
http://tecadmin.net/steps-to-install-java-on-centos-5-6-or-rhel-5-6/
  
===================================================================
If you go with Eclipse, you should consider these plugins
Aptana - either as a plugin or as an alterntive to Eclipse (extra features in commercial)
Mylyn - Task management, often included already. Integrates with lots of other task managers
Goto file - Open files in current project quickly with keyboard
Multi Clipboard - Management of the last X things you cut / pasted
Also, outside of Eclipse, I suggest you get IPython for your command line debugging pleasure. I prefer it over the standard command line shell and IDLE (The GUI shell).
----------------------------------------------------------------------
thrift -gen py:new_style ~/git_repos/hive-0.12-4629.2/service/if/TCLIService.thrift
http://wiki.apache.org/thrift/ThriftUsagePython

generating code in hive from thrift interface - 

thrift --gen java:beans,hashcode -o src/gen/thrift if/TCLIService.thrift
thrift --gen cpp -o src/gen/thrift if/TCLIService.thrift
thrift --gen py -o src/gen/thrift if/TCLIService.thrift
thrift --gen rb -o src/gen/thrift if/TCLIService.thrift

hbase thrift 1 server -
thrift --gen java -o . Hbase.thrift
--------------------------------------------------------
pydev - python IDE for eclipse 3.4.1
eclipse version - 3.8, 4.3 have pydev support

eclipse standard 4.3.1 - kepler SR2(4.3.2), juno, indigo, helios, galileo, ganymede, europa

setting up pydev django eclipse project - 
within git checkout of hue-3.5 - have 
.project
.pydevproject

downloading pydev - 
eclipse -> help -> eclipse market place -> pydev

eclipse -> project ->properties -> pyDev pythonpath -> (tab) string substitution variables


DJANGO_MANAGE_LOCATION
/home/ssatish/git_repos/hue-3.5.0-mapr/desktop/core/ext-py/Django-1.4.5/django/conf/project_template/manage.py
DJANGO_SETTINGS_MODULE
/home/ssatish/git_repos/hue-3.5.0-mapr/desktop/core/ext-py/Django-1.4.5/django/conf/project_template/settings.py

/home/ssatish/git_repos/hue-3.5.0-mapr/build/env/bin/python

traceback.py
threading.py 
needed for pyDev to work properly on ubuntu
/opt/mapr/hue/hue-2.5.0/build/env/lib/python2.6/traceback.py
/opt/mapr/hue/hue-2.5.0/build/env/lib/python2.6/threading.py
/usr/lib/python2.7/traceback.py
/usr/lib/python2.7/threading.py

eclipse pydev step into function calls on mac - 
<fn> + <F3>

open a file - 
<cmd> <shft> R

===================================================================

/opt/mapr/hue/hue-3.5.0/apps/beeswax/gen-py/TCLIService
--------------------
hive example test
/home/ssatish/git_repos/hive-0.12-4629.2/data/files/kv1.txt
/opt/mapr/hive/hive-0.12/examples/files/kv1.txt
SELECT COUNT(*) FROM pokes WHERE foo = 98;
-----------------------------------------------------------------
make -C src zkrest-info
displays info from ecosystem.mk in mercurial hg home trunk
------------------------------------------------------------
mapred.job.tracker=local
local job runner mode where map task and reduce task are self contained within pig jvm,
no separate task jvm  is launched on task tracker node

enable DEBUG_OPTS in bin/pig, (params got from intelliJ remote debugging) so debug server starts listening remotely. connect to it from intelliJ.
 
./pig -Dmapred.job.tracker=local -f ./test.pig
------------------------------------------------------------
pmap - process map - sort by size in column 2 - 
pmap <PID> | sort -r -n -k2
    r = reverse, n= numeric sort rather than alphabetic, k2= column 2
------------------------------------------------------------
create base64 encoded large text file ~256 MB 
openssl rand -out sample.txt -base64 $(( 2**28 * 3/4 ))
2^30 = 1 GB, 2^28 = 256 MB, 3/4 is to account for base64 encoding expansion which is 4/3 times regular plain-text encoding

ant -Dhadoopversion=23 jar-all
Pig compile against yarn MR2
----------------------------------------------------------------------
hue postinst script checks if - 
warden.hue.conf
if [ -f __INSTALL__/desktop/conf/warden.hue.conf ]; then
    cp -Rp __INSTALL__/desktop/conf/warden.hue.conf __PREFIX__/conf/conf.d/.
fi

But who puts it there in the package in the first place?

its done in src/Makefile for target hue, copying from configurations folder which has warden.hue.conf
-----------------------------------------------------------------
--------------------------------------
hive-0.13 build cmd - 
mvn clean install -Phadoop-1,dist -DskipTests

------------------------------------------
run hive unit tests

export MAVEN_OPTS="-Xmx512m -XX:MaxPermSize=256m";
mvn clean install -Phadoop-1,dist -Dmaven.test.failure.ignore=true
-----

top level hive - 
mvn clean package -DskipTests -Phadoop-2 -Pdist

cd itests
mvn clean install -DskipTests -Phadoop-2

cd qtest-spark
mvn test -Dtest=TestSparkCliDriver -Dqfile=groupby4.q -Dtest.output.overwrite=true -Phadoop-2

to see full debug stack trace - use mvn -e and -X
mvn -e -X test -Dtest=TestSparkCliDriver -Dqfile=auto_sortmerge_join_1.q -Dtest.output.overwrite=true -Phadoop-2 > log2
-----------------------------------------
debugging hive - 
Also enable debug in log4j initialization to see which file the Log4J Logger picked up

Export HADOOP_OPTS=-Dlog4j.debug

Start the service
hive --hiveconf hive.log4j.file=metastore-log4j.properties --service metastore

On the terminal log4j prints which settings file it picked up
------------------------------------------------------
unix copy & preserve src heirarchy at destination - 
cp --parents -R ./ql/target/surefire-reports ../hive-uni*
---------------------------------------------------------
terminator 
unix utility to split terminal screen into multiple parts - vikram was using it on his mac

./bin/hive --debug --service hiveserver2

to remote debug hive, comment out this line in bin/hive
export HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"

or else you get this error - 
ERROR: Cannot load this JVM TI agent twice, check your java command line for duplicate jdwp options.
Error occurred during initialization of VM
agent library failed to init: jdwp
---------------------------------------------------------
ant eclipse-files
build eclipse project for hive-0.12
--------------------------------------------
if werkzeug debugger is not installed, on running ipdb, you get
ImportError: No module named werkzeug

to install werkzeug- 
./build/env/bin/pip install Werkzeug

-------------------------------------------------
hue debug with breakpoints - using ipython ipdb - 
insert these 2 lines below where u want to set breakpoint, then start hue with 
DESKTOP_DEBUG=1 ./build/env/bin/hue runserver_plus   (this enables the werkzeug debugger)

import ipdb 
ipdb.set_trace()

at the ipython breakpoint, 
dump() shows all variable values in the frame
dump(obj) dumps all that's known about the object

to print javascript caching in browser - 
DESKTOP_DEPENDER_DEBUG=1 ./build/env/bin/hue runserver_plus
---------------------------------------------

to find the current working directory of a process- 
pwdx 4238
4238: /home/ssatish/git_repos/hive-0.13-mapr
----------
to ping port on remote server ip - port 443 in this example
telnet google.com 443 Trying 74.125.237.19 Connected to google.com. Escape character is '^]'.
---------------
for loop in unix bash   -

for path in $(find ./ -name "workflow.zip"); do
unzip $path; 
done
----------------------------
find ./ -name "work*.*" | zip -@ workflow.zip
-@ makes sure it gets the fill path
------------------------------
JobClient talks to zookeeper to get the job tracker address
-------------------------------------
ssh-keygen -f "/home/ssatish/.ssh/known_hosts" -R 10.10.100.112
remove a machine from list of known_hosts
--------------------------------
copy hbase table to M7 table - 

To copy 'TestTable' to a cluster that uses replication for a 1 hour window:
 $ bin/hbase org.apache.hadoop.hbase.mapreduce.CopyTable --starttime=1265875194289 --endtime=1265878794289 --peer.adr=server1,server2,server3:2181:/hbase --families=myOldCf:myNewCf,cf2,cf3 TestTable 
For performance consider the following general options:
-Dhbase.client.scanner.caching=100
-Dmapred.map.tasks.speculative.execution=false
------------------------------------------------------
./findclass.sh ./ "ThriftServerRunner"
~/scripts/findclass.sh  /opt/mapr/lib/*  "org.datanucleus.jdo.JDOPersistenceManagerFactory"
--------------------------------------------------
compiling hbase - 
mvn clean install package -DskipTests -Dgenerate.mapr.patches -P security
---------------------------------
debugging in javascript - works on chrome browser with inspect element enabled
also works on mozilla firefox browser with firebug plugin enabled

prints in the 'console' window in browser. 
    console.log(handler, "Suhas");
    console.log(arguments, "Satish"); 
        console.log(name + " is finer than fine wine.");
-------------------------------------------------------------
converting a tarball into a git repository - 
git init && git add --ignore-errors .; git commit -m "`basename $PWD`"

http://who-t.blogspot.com/2009/06/git-patches-from-tarballs.html
--------------------
set linux date time - 
sudo date 061117202014.23
format in - 
nnddhhmmyyyy.ss which is described below

nn is a two digit month, between 01 to 12
dd is a two digit day, between 01 and 31, with the regular rules for days according to month and year applying
hh is two digit hour, using the 24-hour period so it is between 00 and 23
mm is two digit minute, between 00 and 59
yyyy is the year; it can be two digit or four digit: your choice. I prefer to use four digit years whenever I can for better clarity and less confusion
ss is two digit seconds. Notice the period ‘.’ before the ss.
--------------------------------------------------------------
man -k kerberos
kadmin (1)           - Kerberos V5 database administration program
kadmin.local (8)     - Kerberos V5 database administration program
kdb5_util (8)        - Kerberos database maintenance utility
kdc.conf (5)         - Kerberos V5 KDC configuration file
kdestroy (1)         - destroy Kerberos tickets
kinit (1)            - obtain and cache Kerberos ticket-granting ticket
klist (1)            - list cached Kerberos tickets
kpasswd (1)          - change a user's Kerberos password
kprop (8)            - propagate a Kerberos V5 principal database to a slave server
kpropd (8)           - Kerberos V5 slave KDC update server
kproplog (8)         - display the contents of the Kerberos principal update log
krb5_newrealm (8)    - Create a new Kerberos Realm
krb5kdc (8)          - Kerberos V5 KDC
ktutil (1)           - Kerberos keytab file maintenance utility
kvno (1)             - print key version numbers of Kerberos principals
--------------------------------------/javap

mapred-site.xml - remote debugging task attempts - 

 <property>
     <name>mapred.child.java.opts</name>
     <value>-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5432</value>
   </property>

<property>
     <name>mapred.map.child.java.opts</name>
     <value>-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5432</value>
   </property>
---------------
debugging map task or reduce child task spawned by pig-
had to put debug logging info in /opt/mapr/hadoop/hadoop*/conf/log4j.properties for pig 
since that log4j.properties file exists on the calsspath of the map task.

log4j.logger.org.apache.pig.impl.io.InterRecordReader=DEBUG
----------
from mcs GUI, all the mapred properties are dispalyed for the submitted job in - 
JobConf: maprfs:/var/mapr/cluster/mapred/jobTracker/staging/root/.staging/job_201407251404_0026/job.xml
---------
download from internet and place unlimited strength jce (java cryptography extension) policy files under jre folder for mapr security to work correctly with kerberos
/usr/java/jdk1.6.0_45/jre/lib/security
--------
/opt/mapr/hadoop/hadoop*/conf/log4j.properties -
to debug authentication issues on secure cluster - 

log4j.logger.org.apache.hadoop.ipc.RPC=DEBUG                          
log4j.logger.org.apache.hadoop.ipc.Client=DEBUG     
----------
grep printing specific line numbers from a file onto stdout - 
sed -n '1234,5555p' someFile

the inverse operation - how to grep and find all the line number that match the character "\N\n"

-------
starting spark master - 
sbin/start-master.sh

starting spark slaves - 
./sbin/start-slaves.sh -p 7078 --webui-port 8080

launching spark-shell 
./bin/spark-shell --master spark://IP:PORT
----------------------
----------------------

./bin/hive --auxpath spark-hadoop.jar
----------
on mac, hive logs are located at 
java.io.tmpdir/${user.name} 

which is 
$TMPDIR/ssatish

whose absolute path is 
/var/folders/zf/zvfr6q7s3cbc56dtzzw5d__m0000gp/T/ssatish/hive.log
----------------------
python debug - print to file 

 f1=open('/home/ssatish/git_repos/ss-hue-mapr/debug', 'w+')
    f1.write('This is a test')
    print >> f1, "deepi : do_as_user = %s " % (request.fs.do_as_user)     
---------------------------
hive partitioning and bucketing concept- 
hive bucketing on iteneraryID (hash column itinID into 2^n buckets) => both tables have # of buckets equal to multiple of each other.

bucketing is usually done on join key  "ON itintiD=x". 

TABLESAMPLE keyword allows random sampling without going thru hash bucketing. it uses block sampling.
sampling strategy:
1) certain %age of data or 2) #rows of data or 3) # bytes of data

TABLESAMPLE(1 PECENT)

partitioned by airline origin state. so each partition is a separate directory on hdfs. partitioning is usually done on "where state=CA"  filter. 
---------------------
show hidden characters in vi editor

press [ctrl]v ctrl[m]  - to get ^M character
1,$s/^M//

to see hidden characters - 
:set list
:set nolist



count number of lines having a pattern - 
:%s/pattern//n

show if a tab separated tsv file is actually tab '\t' character separated or is space \s separated - 
show tab character in vi editor - 
:set listchars=eol:$,tab:>-,trail:~,extends:>,precedes:<
after above, everything which isnt exlicitly shown as a character, is a whitespace \s
---------
ctrl-o steps into links (files) during code navigation in vi editor.

the opposite of it to go back to the previous file is :$

---------
mac print screen snapshot screenshot 
Command+Control+Shift+3
Command+Shift+3

------------
xslt notes to transform xml - 

<xsl:stylesheet version="1.0" xmlns:xsl="http://www.w3.org/1999/XSL/Transform">

  <xsl:output method="xml" version="1.0" encoding="utf-8" indent="yes"/>

  <!-- Identity template: http://en.wikipedia.org/wiki/Identity_transform#Using_XSLT -->
  <xsl:template match="@* | node()">
    <xsl:copy>
      <xsl:apply-templates select="@* | node()"/>
    </xsl:copy>
  </xsl:template>

  <!-- Match any <attribute> element -->
  <xsl:template match="attribute">
    <!--
    Create a new element, using the @name attribute of the current element as
    the element name. The curly braces {} signify attribute value templates:

    http://lenzconsulting.com/how-xslt-works/#attribute_value_templates
    -->
    <xsl:element name="{@name}">
      <!-- Use the value of the @value attribute as the content of the new element -->
      <xsl:value-of select="@value"/>
    </xsl:element>
  </xsl:template>

</xsl:stylesheet>
-----------------
linux check available memory - 
free -m
-----
javascript - 

var countBooleans = function(b1, b2, b3) {
    return [ "None of the flags are set", "One of the flags is set", "Two of the flags are set",  "Three of the flags are set" ]
           [ !!b1 + !!b2 + !!b3 ] ;
  };

The !! are just to guarantee that the values really are booleans.  The trick is that in Javascript, addition will cast booleans as ints, specifically to 0 and 1 for false and true.
----------
mac osx copy to clipboard from command line read of a file  -
pbcopy < ~/.ssh/id_rsa.pub

fswatch -Ee '/Users/ssatish/Projects/Ventana/revenge/(\.git|\.idea|log|vendor)/' -o -l 0.2 /Users/ssatish/Projects/Ventana/revenge | xargs -n1 -I{} /Users/ssatish/Projects/Ventana/revenge/script/code_sync/sync_revenge.rb den-smash3.ch.int shared/revenge-sync false >> /Users/ssatish/Projects/Ventana/revenge/log/code_sync-den-smash3.ch.int_shared-revenge-sync_false.log 2>&1 &
--------------------------
encoding problems - 
iconv -f ISO-8859-1 -t UTF-8 flight_times_wnull.csv > flight_times_wnull_utf8.csv
iconv -f UTF-8 -t UTF-8 flight_times_wnull.csv > flight_times_wnull_utf8.csv

convert from 1 encoding type to another on mac/linux cmd line 

os x compatible version to do bulk convert with iconv across directory heirarchies - 
find . -type f -exec bash -c 'iconv -f iso-8859-1 -t utf-8 "{}" > /path/to/destination/"{}"' \;
-------------
photos pics pictures from iphone are backed up on mac os x computer PC at - 
~/Library/Application Support/MobileSync/Backup/

~/Pictures/Photos Library.photoslibrary/


and primarily at - 
~/Pictures/Photos Library.photoslibrary/

mac os x - in finder (GUI directory browser), short cut to go to a specific folder is - 
cmd shift G

mac os x - show hidden windows which are moving out of the screen - 
press F3 and then when hidden window appears, drag it into a desktop_view to pin it in visible & mouse-accessible area

os x search using finder on cmd line - 
mdfind -onlyin ~/Documents essay 

screencapture -C -M image.png 


cmd-shift-4 - this turns your mouse into a selection tool from which you can select images in browser or anywher eon the screen, and a screenshot is taken and stored
 in your desktop. Helpful if you're trying to replicate website content like logos in your web page.
--------------
bash history with timestamp - 
echo 'export HISTTIMEFORMAT="%d/%m/%y %T "' >> ~/.bashrc

--------------
mac os x - microsoft excel 2011 - unpivot summary table into a database table - 

alt cmd p


excel for mac os x - freeze first row upon scrolling -
Select the Layout tab from the toolbar at the top of the screen. Click on the Freeze Panes button and click on the Freeze Top Row option in the popup menu.
--------------------------
#replacing all \newline characters using gnu sed with a white space - 

sed -e ':a' -e 'N' -e '$!ba' -e 's/\\\n/ /g' head_location_geocodes.tsv.orig >  head_location_geocodes.tsv.newline 

sed -i ‘9s/{old_schema}/{new_schema}/‘ <sql_dump_file_name>

Explanation:
Create a label via :a.
Append the current and next line to the pattern space via N.
If we are before the last line, branch to the created label $!ba ($! means not to do it on the last line as there should be one final newline).
Finally the substitution replaces every newline with a space on the pattern space (which is the whole file).

even the below cmd works instead of the one above but below works only in gnu sed (linux distros) but not BSD sed (which is used by mac os x, cygwin)- 
cat head_location_geocodes.tsv.orig | sed ':a;N;$!ba;s/\\\n/ /g' > head_location_geocodes.tsv.parallel

Next, try to make it work in parallel to speed up operations on very large files ~56 GB
hint:
cat {0} | parallel --keep-order --gnu --pipe {some_magic_cmd}
-----------------
google chrome reading list - 
http://www.herbrich.me/papers/adclicksfacebook.pdf

http://hortonworks.com/blog/using-pagerank-detect-anomalies-fraud-healthcare/

https://drive.google.com/file/d/0BzXCis10x6hKV0ZHUTJWbEhpYk0/view
------------------
cat /etc/*-release
#to know which linux distribution it is
-----------
in unix/linux or mac os X , to see the top memory-hogging processes, use (default is descending)
top -o mem


-------
word count with top-k occurences of a word using unix pipes - 
 awk '{print $7}' access.log |
        sort      |
        uniq -c   |
        sort -rn  |
        head -n 5
replaces duplicate occurences with wword, count , reverse sort it and then take the top 5

aws s3 ls --recursive s3://krux-audience-segments/daily-user-segment/ | grep "2017-12-05" | awk '{print $4}' | awk -F "/" '{print $2}' | uniq

-------
to see how many unique cms_ids we have in the ska dataset,
241,879

cat OM353526_12-20-2016_Final.CSV | awk -F "," '{print $42}' | sort | uniq | wc -l
5,474 unique cms_ids

5,650 uniq cms_ids
----------------
mac online paintbrush (since you need admin privileges to install macpaint thru xcode which cslt doesnt have in self-service)
https://www.youidraw.com/apps/painter/#
-------
solving one problem from each of DP + Disjoint Sets , DP + Combinatorics , and DP + Graph is better than solving three random Dynamic Programming problems. -weak learner.net
-----------
spell checker enable in vi editor - :set spell
--------
force quit applications on mac os x - like ctrl-alt-delete on windows - 
cmd-alt-esc

to push from dee's laptop - 
git push https://github.com/suhassatish/algorithms master:master
-------------
mac os x - disk inventory X tool - to free up hard disk space

amazon to buy  - western digital 500 GB  - USB 3.0 portable external hard disc - $50 ($10 off on amazon)

wondershare filmora - check student discount rate
compare with $19 for adobe premiere pro ($19)
videos created here - 
/Users/deepika/Movies/Wondershare\ Filmora/Uplad/*.mp4
-------
enabling NTFS hard disk mount partition drive write support from OS X yosemite - 
diskutil info /Volumes/DRIVENAME | grep UUID
sudo -i
echo "UUID=63F79D8E-2EE7-49D6-9406-05FA00D06821 none ntfs rw,auto,nobrowse" >> /etc/fstab

(DO NOT RENAME VOLUMES. IT MAKES THE CONTENTS INVISIBLE. How to restore its view?)
------------
diskutil list
lists all volumes on mac and their mount types and partitions (ext2, ext3, ext4, ntfs, fat32, exFat, HFS (apple), etc
--------------
Example: diskutil mergePartitions JHFS+ NewName disk3s4 disk3s7
         This example will merge all partitions *BETWEEN* disk3s4 and disk3s7,
         preserving data on disk3s4 but destroying data on disk3s5, disk3s6,
         disk3s7 and any invisible free space partitions between those disks;
         disk3s4 will be grown to cover the full space if possible.
diskutil mergePartitions exFAT Vaio120G disk4s1 disk4s3
------------------------------
Suddenly all my files disappeared - please help!

This is usually happen when not all files are written properly due to an unmount operation not finishing. The NTFS partition might be marked as "dirty" and the Apple NTFS driver cannot recover from that situation. Mounty will not delete anything by itself, please try to restore your files on a Windows PC using usual recovery s/w, i.e. chkdsk command line utility or professional tools like GetDataBack for Windows. If you do not own any Windows you can use tools for macOS that can deal with NTFS partition maintenance, like Paraogn Harddisk Manager or Tuxera Disk Manager.
-------------------------------
notes specific to ubuntu below - 
ifconfig | grep inet
# displays the ip address of  a machine

-----
bin/zookeeper.sh 
-------------------
ps -ax | grep -i intellij | awk '{print $1}' | xargs sudo kill -9
---------------
gmail show large emails occupying most storage space - 
---------
mac terminal not starting in home dir /Users/ssatish but in some /var/ volume. To fix it - 

https://apple.stackexchange.com/questions/305627/what-is-causing-high-sierra-to-forget-where-my-home-directory-is
last comment. 

I found the following to "correct" the issue.

Open System Preferences > Users & Groups
Unlock the pane
Right click on the effected user account > Advanced Options...
Next to Home directory: click Choose...
Click Open
Click OK
This seems to have re-written the home directory path and allowed functionality to return.

In my case, logging out and back in resulted in a black screen with a cursor - though after a soft reboot able to log back in, long progress bar then everything was back to normal.

then restart terminal.
---------------------------
sudo -u spark jstat -gcutil  8868
sudoing as user "spark" and running jstat with -gcutil option for checking garbage collection on a process with PID = 8868
----------------
avro to json conversion - download avrotools.jar from the avro apache open source project page. Alternatively, brew install avro-tools and then run avro-tools to get cmds like `fromjson`, `tojson`, etc
java -jar avrotools.jar fromjson --schema-file blah.avsc data.json
-----
jq - cmdline json processor

for ID in $(curl -XGET --silent 'http://ec-platform-emr5.7-shared-prod.sfiqplatform.com:8088/ws/v1/cluster/apps?states=running' 
  | jq -c '.apps.app[]
  |[.name,.id,.elapsedTime]' 
  | sed -e 's/^\[//' -e 's/\]$//' 
  | grep DataPuller 
  | awk -F , '{if ($3 > 7200000) { print $2; }}' 
  | sed 's/"//g'); do curl --silent -XPUT -H "Content-Type: application/json" --data '{"state":"KILLED"}' 'http://ec-platform-emr5.7-shared-prod.sfiqplatform.com:8088/ws/v1/cluster/apps/'$ID'/state'; done

------
eclipse

cmd-shift-F = auto format aura markup .cmp file
----------------
os.popen('VBoxManage -v 2>/dev/null').read().strip()
//read from stdout in python after launching an OS process
----------------
foo() {
  DIR=~/projects/builder/script/parsing
    
    cat $1 | ${DIR}/extract_op_log.py | ${DIR}/flatten_flog.py -p derivedFeatureValue contribution  | ${DIR}/explode_arr_col.py --c contribution | sort -t '|' -rgk 4 | awk -F'|' '{print $4 "\t" $3 "\t" $2 "\t" $1}' | less

join example of 2 log data sets in unix - 
join -1 3 -2 3 <(contrib stagingTrain_RF_FilterLabel_1000.log) <(contrib stagingTrain_LR_FilterLabel_1000.log)
------------------

. ~/blt/env.sh; p4 client -o | sed 's/ noclobber/ clobber/' | p4 client -i

root      /opt/cisco/hostscan/bin/ciscod -d
root      /opt/cisco/anyconnect/bin/vpnagentd -execv_instance
ssatish   /opt/cisco/hostscan/bin/cscan
--------
docker build -t test_container .
